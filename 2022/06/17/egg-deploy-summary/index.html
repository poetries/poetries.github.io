<!DOCTYPE html>


  <html class="dark page-post">


<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>docker-compose/微信云托管/serverless之部署Egg项目 | 前端进阶之旅</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="部署,Egg,">
  

  <meta name="description" content="一、本地docker环境搭建mac下安装docker: brew install docker  https://hub.docker.com 拉取镜像速度比较慢，我们推荐使用国内的镜像源访问速度较快 https://hub.daocloud.io  1.1 设置国内镜像源 &amp;#123;  &quot;registry-mirrors&quot;: [    &quot;https://register.docker-cn.c">
<meta name="keywords" content="部署,Egg">
<meta property="og:type" content="article">
<meta property="og:title" content="docker-compose&#x2F;微信云托管&#x2F;serverless之部署Egg项目">
<meta property="og:url" content="http://blog.poetries.top/2022/06/17/egg-deploy-summary/index.html">
<meta property="og:site_name" content="前端进阶之旅">
<meta property="og:description" content="一、本地docker环境搭建mac下安装docker: brew install docker  https://hub.docker.com 拉取镜像速度比较慢，我们推荐使用国内的镜像源访问速度较快 https://hub.daocloud.io  1.1 设置国内镜像源 &amp;#123;  &quot;registry-mirrors&quot;: [    &quot;https://register.docker-cn.c">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/82cdc649caaa9a4d.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/59d83f0139c878de.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/7b993abfdc8f07b6.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/df79f11b704aae4d.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/566d2f2270c40840.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/129f0a59b6d7c5d4.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/a6a34ed8d66f7cdc.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/f7e34f9ea2331ffb.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/5ba00266271ef558.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/16c4541cf84b6e48.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/65357b17bf38f12e.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/639c44fb75dfe354.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/2ce28bda82ab01d4.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/b38b5f228f3bcafe.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/57a4ce88939753b0.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/b6183fc0a23c353d.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/ab4cf5d68cdd846b.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/49bb3cc9f49c2dd1.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/1f9aea9f1c9a18fc.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/691129b326f9fb23.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/6d9b07b85c22e0b5.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/d0ded5a28d96ce59.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/cb0109ef19b551e4.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/1332f6a87ae0879f.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/a799a68a83398d4b.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/e8a4d267eb9342c1.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/85c0f4122baf860d.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/8ec0512b8a2a0652.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/4a710e9a2a883e24.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/0bb97a6d629301ac.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/e0f4f8f2621b11c0.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/0b017f9a76914c6e.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/dad88a1878a7a12f.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/fb606d9823c4f0ff.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/3a9f7618e1baf34f.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/e4657b28bcea9b61.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/bcaf72ea48991732.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/1bf0445f94a779a6.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/2744890d4980cfc8.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/0327d670f6d365a3.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/1b6745e93aa9117b.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/bc579093039d5dff.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/e0867be8b406e212.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/b2cacc63e98f5a06.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/1f3bfb1ae436d23a.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/eda7643690d38f9d.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/52c4f5f008df9f52.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/ca3f547b33538895.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/575fadaac0763dc6.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/54f7c259eaf6172c.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/dc8e99cf5574f3fd.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/64b59c23ed0cc0d7.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/c74e6e89043a20d6.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/1e17df55d9c3634f.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/144606d89bc20359.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/c09974e946ab3075.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/71c1f38043c67c1b.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/2c6ce688f421ef04.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/0f3919c9924b6071.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/3e1b91db0e2b0718.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/d0dd17b85fd1e305.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/e3a9c648993208d2.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/713c1e6256b93b9a.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/c3b97b3e633cb928.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/91202ab620ae6b65.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/41b9fcbf9242921f.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/bdea8f2beb76abc4.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/471ac2511396e898.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/51a5bdc6a3e2ee10.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/fbf0b03c839cd73e.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/58b123659a00b194.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/c4c3e330b1a6af68.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/85b3d73ea922f0ea.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/a96ce1c3799f5951.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/ce48d9dd73af824f.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/9a0e3dfa25a6ed3d.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/e9dd1225bf0d37cd.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/ca238c35541dd8df.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/4ab13bbb74bcdf89.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/5a4f88d78a4740fb.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/692ad2fc22a7721d.png">
<meta property="og:image" content="https://s.poetries.work/uploads/2022/06/38120fb475238537.png">
<meta property="og:updated_time" content="2025-03-30T13:54:29.458Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="docker-compose&#x2F;微信云托管&#x2F;serverless之部署Egg项目">
<meta name="twitter:description" content="一、本地docker环境搭建mac下安装docker: brew install docker  https://hub.docker.com 拉取镜像速度比较慢，我们推荐使用国内的镜像源访问速度较快 https://hub.daocloud.io  1.1 设置国内镜像源 &amp;#123;  &quot;registry-mirrors&quot;: [    &quot;https://register.docker-cn.c">
<meta name="twitter:image" content="https://s.poetries.work/uploads/2022/06/82cdc649caaa9a4d.png">

  

  
    <link rel="icon" href="/favicon.ico">
  

  <link href="/css/styles.css?v=c114cbeddx" rel="stylesheet">
<link href="/css/other.css?v=c114cbeddx" rel="stylesheet">


  
    <link rel="stylesheet" href="/css/personal-style.css">
  

  

  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?5081f3afc8d94338e79d319c8b632b31";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  
  <script type="text/javascript">
	(function(){
	    var bp = document.createElement('script');
	    var curProtocol = window.location.protocol.split(':')[0];
	    if (curProtocol === 'https') {
	        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
	    }
	    else {
	        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
	    }
	    var s = document.getElementsByTagName("script")[0];
	    s.parentNode.insertBefore(bp, s);
	})();
  </script>



  
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">
  

  <!-- 聊天系统 -->
  
    
   <link type="text/css" rel="stylesheet" href="/renxi/default.css">
   <style>
      #modal {
        position: static !important;
      }
      .filter {
        width: 100%;
        height: 100%;
        position: absolute;
        top: 0;
        left: 0;
        background: #fe5757;
        animation: colorChange 30s ease-in-out infinite;
        animation-fill-mode: both;
        mix-blend-mode: overlay;
      }
  
      @keyframes colorChange {
        0%, 100% {
            opacity: 0;
        }
        50% {
            opacity: .9;
        }
      }
   </style>
</head>
</html>
<body>
  
  
    <span id="toolbox-mobile" class="toolbox-mobile">导航</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">导航</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/archives/"
            rel="noopener noreferrer"
            target="_self"
            >
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/categories/"
            rel="noopener noreferrer"
            target="_self"
            >
            分类
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/tags/"
            rel="noopener noreferrer"
            target="_self"
            >
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/search/"
            rel="noopener noreferrer"
            target="_self"
            >
            搜索
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/link/"
            rel="noopener noreferrer"
            target="_self"
            >
            友链
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/about/"
            rel="noopener noreferrer"
            target="_self"
            >
            关于
          </a>
        </li>
      
    </ul>
  </div>


</div>


  <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录<i class="iconfont toc-title" style="display:inline-block;color:#87998d;width:20px;height:20px;">&#xf004b;</i></strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#一、本地docker环境搭建"><span class="toc-text">一、本地docker环境搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-设置国内镜像源"><span class="toc-text">1.1 设置国内镜像源</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-docker命令基础"><span class="toc-text">1.2 docker命令基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-环境准备"><span class="toc-text">1.3 环境准备</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、安装node镜像"><span class="toc-text">1、安装node镜像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、安装MySQL镜像"><span class="toc-text">2、安装MySQL镜像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、安装redis镜像"><span class="toc-text">3、安装redis镜像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4、安装Nginx镜像"><span class="toc-text">4、安装Nginx镜像</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-部署egg代码"><span class="toc-text">1.4 部署egg代码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#启动egg镜像"><span class="toc-text">启动egg镜像</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#二、docker-compose部署"><span class="toc-text">二、docker-compose部署</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-编写docker-compose-yml文件"><span class="toc-text">2.1 编写docker-compose.yml文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-启动服务"><span class="toc-text">2.2 启动服务</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#三、Nginx容器内部署前端"><span class="toc-text">三、Nginx容器内部署前端</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#四、docker部署到云服务器"><span class="toc-text">四、docker部署到云服务器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-安装docker环境"><span class="toc-text">4.1 安装docker环境</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#安装工具包"><span class="toc-text">安装工具包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#设置阿里镜像源"><span class="toc-text">设置阿里镜像源</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#安装docker"><span class="toc-text">安装docker</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#启动docker"><span class="toc-text">启动docker</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#设置docker镜像源"><span class="toc-text">设置docker镜像源</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#安装mysql镜像测试"><span class="toc-text">安装mysql镜像测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-安装docker-compose"><span class="toc-text">4.2 安装docker-compose</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-开放服务器端口"><span class="toc-text">4.3 开放服务器端口</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-部署egg项目"><span class="toc-text">4.4 部署egg项目</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#修改代码和配置"><span class="toc-text">修改代码和配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#上传本地egg服务端代码到服务器"><span class="toc-text">上传本地egg服务端代码到服务器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#启动egg服务"><span class="toc-text">启动egg服务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#测试服务"><span class="toc-text">测试服务</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#五、部署到云托管"><span class="toc-text">五、部署到云托管</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-redis服务"><span class="toc-text">5.1 redis服务</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-mysql服务"><span class="toc-text">5.2 mysql服务</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-egg部署"><span class="toc-text">5.3 egg部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#修改代码"><span class="toc-text">修改代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#新建服务"><span class="toc-text">新建服务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#调试接口"><span class="toc-text">调试接口</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#六、egg部署到腾讯serverless"><span class="toc-text">六、egg部署到腾讯serverless</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-修改egg配置"><span class="toc-text">6.1 修改egg配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-命令行部署"><span class="toc-text">6.2 命令行部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#配置-YAML"><span class="toc-text">配置 YAML</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#部署到腾讯云"><span class="toc-text">部署到腾讯云</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#移除"><span class="toc-text">移除</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#账号配置（可选）"><span class="toc-text">账号配置（可选）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#注意！！！"><span class="toc-text">注意！！！</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-控制台创建部署-模板部署"><span class="toc-text">6.3 控制台创建部署-模板部署</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-控制台创建部署-自定义部署"><span class="toc-text">6.4 控制台创建部署-自定义部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#初始化项目"><span class="toc-text">初始化项目</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#部署上云"><span class="toc-text">部署上云</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-5-测试接口"><span class="toc-text">6.5 测试接口</span></a></li></ol></li></ol>
  </div>
  




<div class="content content-post CENTER">
   <!-- canvas 彩带 -->
<canvas id="evanyou" width="1302" height="678" style="position: fixed;width: 100%;height: 100%;top: 0;left:0;z-index:-1;"></canvas>

<div class="qrcode_container">
  <div class="tencent_code">
    <h4>关注作者公众号</h4> 
    <p>和万千小伙伴一起学习</p> 
    <img src="https://interview.poetries.top/qrcode.jpg" alt="公众号：前端进价之旅">
  </div> 
</div>

<article id="post-egg-deploy-summary" class="article article-type-post" itemprop="blogPost">
  <header class="article-header" style="position:relative;">
    <h1 class="post-title">docker-compose/微信云托管/serverless之部署Egg项目</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2022.06.17</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>Poetry</span>
        </span>
      

      
  <span class="article-category">
    <i class="icon-list"></i>
    <a class="article-category-link" href="/categories/Front-End/">Front-End</a>
  </span>



      

      
      <i class="fa fa-eye"></i> 
        <span id="busuanzi_container_page_pv">
           &nbsp热度 <span id="busuanzi_value_page_pv">
           <i class="fa fa-spinner fa-spin"></i></span>℃
        </span>
      
      
       
          <span class="post-count">
            <i class="fa fa-file-word-o"></i>&nbsp
            <span>字数统计 18.6k字</span>
          </span>

          <span class="post-count">
            <i class="fa fa-columns"></i>&nbsp
            <span>阅读时长 105分</span>
          </span>
      
      
    </div>

    <i class="iconfont" id="toc-eye" style="display:inline-block;color:#b36619;position:absolute;top:20px;right:-11px;cursor:pointer;
    font-size: 24px;">&#xe61c;</i>

  </header>

  <div class="article-content">
    
      <div id="container">
        <h1 id="一、本地docker环境搭建"><a href="#一、本地docker环境搭建" class="headerlink" title="一、本地docker环境搭建"></a>一、本地docker环境搭建</h1><p>mac下安装docker: <code>brew install docker</code></p>
<blockquote>
<p><a href="https://hub.docker.com" target="_blank" rel="noopener">https://hub.docker.com</a> 拉取镜像速度比较慢，我们推荐使用国内的镜像源访问速度较快 <a href="https://hub.daocloud.io" target="_blank" rel="noopener">https://hub.daocloud.io</a></p>
</blockquote>
<h2 id="1-1-设置国内镜像源"><a href="#1-1-设置国内镜像源" class="headerlink" title="1.1 设置国内镜像源"></a>1.1 设置国内镜像源</h2><p><img src="https://s.poetries.work/uploads/2022/06/82cdc649caaa9a4d.png" alt></p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"registry-mirrors"</span>: [</span><br><span class="line">    <span class="string">"https://register.docker-cn.com/"</span></span><br><span class="line">  ],</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>进入该网站<code>https://hub.daocloud.io</code>获取镜像的下载地址</p>
<h2 id="1-2-docker命令基础"><a href="#1-2-docker命令基础" class="headerlink" title="1.2 docker命令基础"></a>1.2 docker命令基础</h2><ul>
<li><code>docker images</code> 查看镜像</li>
<li><code>docker ps</code> 查看启动的容器 (<code>-a</code> 查看全部)</li>
<li><code>docker rmi 镜像ID</code> 删除镜像</li>
<li><code>docker rm 容器ID</code> 删除容器</li>
<li><code>docker exec -it 1a8eca716169(容器ID:docker ps获取) sh</code> 进入容器内部</li>
<li><code>docker inspect bf70019da487(容器ID)</code> 查看容器内的信息 </li>
</ul>
<blockquote>
<p>删除none的镜像，要先删除镜像中的容器。要删除镜像中的容器，必须先停止容器。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker rmi $(docker images | grep &quot;none&quot; | awk &apos;&#123;print $3&#125;&apos;)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker stop $(docker ps -a | grep &quot;Exited&quot; | awk &apos;&#123;print $1 &#125;&apos;) //停止容器</span><br><span class="line"></span><br><span class="line">$ docker rm $(docker ps -a | grep &quot;Exited&quot; | awk &apos;&#123;print $1 &#125;&apos;) //删除容器</span><br><span class="line"></span><br><span class="line">$ docker rmi $(docker images | grep &quot;none&quot; | awk &apos;&#123;print $3&#125;&apos;) //删除镜像</span><br></pre></td></tr></table></figure>
<h2 id="1-3-环境准备"><a href="#1-3-环境准备" class="headerlink" title="1.3 环境准备"></a>1.3 环境准备</h2><p>这里拉取<code>nginx</code>、<code>node</code>、<code>redis</code>、<code>mysql</code>镜像</p>
<h3 id="1、安装node镜像"><a href="#1、安装node镜像" class="headerlink" title="1、安装node镜像"></a>1、安装node镜像</h3><p>进入<code>https://hub.daocloud.io</code> 搜索node，切换到版本获取下载地址</p>
<ul>
<li><code>docker pull daocloud.io/library/node:12.18</code></li>
<li><code>docker tag 28faf336034d node</code> 重命名镜像</li>
</ul>
<p>重命名镜像后IMAGE ID都是一样的</p>
<p><img src="https://s.poetries.work/uploads/2022/06/59d83f0139c878de.png" alt></p>
<p>也可以导出镜像到本地备份 <code>docker save -o node.image(导出镜像要起的名称) 28faf336034d(要导出的镜像的ID)</code></p>
<p><img src="https://s.poetries.work/uploads/2022/06/7b993abfdc8f07b6.png" alt></p>
<p>我们先删除之前的镜像 <code>docker rmi 28faf336034d -f</code> 强制删除</p>
<p><img src="https://s.poetries.work/uploads/2022/06/df79f11b704aae4d.png" alt></p>
<p>再次导入本地镜像</p>
<p><code>docker load -i node.image(导入的镜像名称)</code></p>
<p><img src="https://s.poetries.work/uploads/2022/06/566d2f2270c40840.png" alt></p>
<p>然后再次重命名镜像即可</p>
<p><code>docker tag 28faf336034d node:v1.0(版本v1.0)</code></p>
<p><img src="https://s.poetries.work/uploads/2022/06/129f0a59b6d7c5d4.png" alt></p>
<h3 id="2、安装MySQL镜像"><a href="#2、安装MySQL镜像" class="headerlink" title="2、安装MySQL镜像"></a>2、安装MySQL镜像</h3><p>进入<code>https://hub.daocloud.io</code> 搜索mysql，切换到版本获取下载地址</p>
<p><img src="https://s.poetries.work/uploads/2022/06/a6a34ed8d66f7cdc.png" alt></p>
<ul>
<li><code>docker pull daocloud.io/library/mysql:8.0.20</code></li>
</ul>
<p><img src="https://s.poetries.work/uploads/2022/06/f7e34f9ea2331ffb.png" alt></p>
<p><strong>启动MySQL镜像</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -d(后台运行) -p 3307:3306(本机端口:MySQL运行端口) --name mysql(容器名称) -e MYSQL_ROOT_PASSWORD=123456(设置mysql密码) be0dbf01a0f3(mysql镜像ID)</span><br></pre></td></tr></table></figure>
<p><strong>查看当前正在运行的镜像</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker ps -a(正在运行和停止的镜像-a都可见)</span><br></pre></td></tr></table></figure>
<p><img src="https://s.poetries.work/uploads/2022/06/5ba00266271ef558.png" alt></p>
<p><strong>删除容器</strong></p>
<p>删除之前需要stop：<code>docker stop bac2692e2b9a(容器ID)</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker rm bac2692e2b9a(容器ID：docker ps获取)</span><br></pre></td></tr></table></figure>
<p><strong>进入容器内部</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker exec -it bac2692e2b9a(容器ID) sh(指定进入方式)</span><br></pre></td></tr></table></figure>
<p><img src="https://s.poetries.work/uploads/2022/06/16c4541cf84b6e48.png" alt></p>
<p>我们使用Navicat新建一个连接测试一下</p>
<p><img src="https://s.poetries.work/uploads/2022/06/65357b17bf38f12e.png" alt></p>
<p>说明我们使用docker安装MySQL的方式是没问题的</p>
<p><strong>查看MySQL容器日志</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker logs -f(查看最后几条)  bac2692e2b9a(容器ID)</span><br></pre></td></tr></table></figure>
<p><img src="https://s.poetries.work/uploads/2022/06/639c44fb75dfe354.png" alt></p>
<p><strong>重启容器</strong></p>
<p>如果修改了容器配置，我们需要重新启动容器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker restart bac2692e2b9a(容器ID)</span><br></pre></td></tr></table></figure>
<p><strong>设置MySQL权限</strong></p>
<blockquote>
<p>mysql8.0后，需要设置，否则node连接不上</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker exec -it bac2692e2b9a sh</span><br><span class="line"></span><br><span class="line">mysql -uroot -p</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 远程连接权限</span></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO <span class="string">'root'</span>@<span class="string">'%'</span> WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 刷新权限</span></span><br><span class="line">FLUSH PRIVILEGES;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新加密规则</span></span><br><span class="line">ALTER USER <span class="string">'root'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'password'</span> PASSWORD EXPIRE NEVER;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新用户密码</span></span><br><span class="line">ALTER USER <span class="string">'root'</span>@<span class="string">'localhost'</span> IDENTIFIED WITH mysql_native_password BY <span class="string">'123456'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 刷新权限</span></span><br><span class="line">FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure>
<p><img src="https://s.poetries.work/uploads/2022/06/2ce28bda82ab01d4.png" alt></p>
<h3 id="3、安装redis镜像"><a href="#3、安装redis镜像" class="headerlink" title="3、安装redis镜像"></a>3、安装redis镜像</h3><p><img src="https://s.poetries.work/uploads/2022/06/b38b5f228f3bcafe.png" alt></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull daocloud.io/library/redis:6.0.3-alpine3.11</span><br></pre></td></tr></table></figure>
<p><strong>启动Redis镜像</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -d -p 6380:6379 --name redis 29c713657d31(镜像ID) --requirepass 123456(redis登录密码)</span><br></pre></td></tr></table></figure>
<p><img src="https://s.poetries.work/uploads/2022/06/57a4ce88939753b0.png" alt></p>
<p>或进入redis镜像后在输入密码</p>
<p><img src="https://s.poetries.work/uploads/2022/06/b6183fc0a23c353d.png" alt></p>
<p><strong>交互式进入redis容器</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker exec -it 9751cbc96861(容器ID) sh</span><br></pre></td></tr></table></figure>
<p><img src="https://s.poetries.work/uploads/2022/06/ab4cf5d68cdd846b.png" alt></p>
<h3 id="4、安装Nginx镜像"><a href="#4、安装Nginx镜像" class="headerlink" title="4、安装Nginx镜像"></a>4、安装Nginx镜像</h3><p><img src="https://s.poetries.work/uploads/2022/06/49bb3cc9f49c2dd1.png" alt></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull daocloud.io/library/nginx:1.13.0-alpine</span><br></pre></td></tr></table></figure>
<p><img src="https://s.poetries.work/uploads/2022/06/1f9aea9f1c9a18fc.png" alt></p>
<p><strong>启动Nginx镜像</strong></p>
<p>服务器上启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run --name nginx(起一个容器名称) -d(后台运行) -p 80:80(本机:容器) -v(映射Nginx容器的运行目录本机) /root/nginx/log:/var/log/nginx(本机目录:容器目录) -v /root/nginx/conf/nginx.conf:/etc/nginx/nginx.conf(本机目录:容器内nginx配置所在目录) -v /root/nginx/conf.d:/etc/nginx/conf.d -v /root/nginx/html:/usr/share/nginx/html f00ab1b3ac6d(nginx镜像ID)</span><br></pre></td></tr></table></figure>
<p>本地电脑启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run --name nginx -d -p 8666:80 -v /Users/poetry/Downloads/docker/nginx/log:/var/log/nginx -v /Users/poetry/Downloads/docker/nginx/conf/nginx.conf:/etc/nginx/nginx.conf -v /Users/poetry/Downloads/docker/nginx/conf.d:/etc/nginx/conf.d -v /Users/poetry/Downloads/docker/nginx/html:/usr/share/nginx/html f00ab1b3ac6d</span><br></pre></td></tr></table></figure>
<blockquote>
<p>把docker容器中的Nginx服务配置映射本地方便管理</p>
</blockquote>
<p><img src="https://s.poetries.work/uploads/2022/06/691129b326f9fb23.png" alt></p>
<p>访问docker暴露的8666端口即可</p>
<p><img src="https://s.poetries.work/uploads/2022/06/6d9b07b85c22e0b5.png" alt></p>
<p>当我们修改了html中的文件，无需重启容器即可看到效果</p>
<p><img src="https://s.poetries.work/uploads/2022/06/d0ded5a28d96ce59.png" alt><br><img src="https://s.poetries.work/uploads/2022/06/cb0109ef19b551e4.png" alt></p>
<h2 id="1-4-部署egg代码"><a href="#1-4-部署egg代码" class="headerlink" title="1.4 部署egg代码"></a>1.4 部署egg代码</h2><blockquote>
<p>构建egg镜像，进入到egg目录</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 构建egg镜像，版本v1.0</span><br><span class="line">docker build -t egg:v1.0 .</span><br></pre></td></tr></table></figure>
<p><code>Dockerfile文件如下</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用node镜像</span></span><br><span class="line">FROM daocloud.io/library/node:12.18</span><br><span class="line"><span class="comment"># 在容器中新建目录文件夹 egg</span></span><br><span class="line">RUN mkdir -p /egg</span><br><span class="line"><span class="comment"># 将 /egg 设置为默认工作目录</span></span><br><span class="line">WORKDIR /egg</span><br><span class="line"><span class="comment"># 将 package.json 复制默认工作目录</span></span><br><span class="line">COPY package.json /egg/package.json</span><br><span class="line"><span class="comment"># 安装依赖</span></span><br><span class="line">RUN yarn config <span class="built_in">set</span> register https://registry.npm.taobao.org</span><br><span class="line"><span class="comment"># 只安装dependencies的包</span></span><br><span class="line">RUN yarn --production</span><br><span class="line"><span class="comment"># 再copy代码至容器</span></span><br><span class="line">COPY ./ /egg</span><br><span class="line"><span class="comment"># 7001端口</span></span><br><span class="line">EXPOSE 7001</span><br><span class="line"><span class="comment"># 等容器启动之后执行脚本</span></span><br><span class="line">CMD yarn prod</span><br></pre></td></tr></table></figure>
<h3 id="启动egg镜像"><a href="#启动egg镜像" class="headerlink" title="启动egg镜像"></a>启动egg镜像</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -d(后台启动) -p 7001:7001(本机:容器) --name server(容器名称) af9360186a24(镜像ID)</span><br></pre></td></tr></table></figure>
<p><img src="https://s.poetries.work/uploads/2022/06/1332f6a87ae0879f.png" alt></p>
<h1 id="二、docker-compose部署"><a href="#二、docker-compose部署" class="headerlink" title="二、docker-compose部署"></a>二、docker-compose部署</h1><h2 id="2-1-编写docker-compose-yml文件"><a href="#2-1-编写docker-compose-yml文件" class="headerlink" title="2.1 编写docker-compose.yml文件"></a>2.1 编写docker-compose.yml文件</h2><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">"3.0"</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span> </span><br><span class="line"><span class="attr">    redis:</span> <span class="comment"># 服务名称</span></span><br><span class="line"><span class="attr">        container_name:</span> <span class="string">redis</span> <span class="comment"># 容器名称</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">daocloud.io/library/redis:6.0.3-alpine3.11</span> <span class="comment"># 使用官方镜像</span></span><br><span class="line"><span class="attr">        ports:</span> </span><br><span class="line"><span class="bullet">            -</span> <span class="number">6380</span><span class="string">:6379</span> <span class="comment"># 本机端口:容器端口</span></span><br><span class="line"><span class="attr">        restart:</span> <span class="string">on-failure</span> <span class="comment"># 自动重启</span></span><br><span class="line"><span class="attr">        networks:</span> </span><br><span class="line"><span class="bullet">            -</span> <span class="string">my-server</span></span><br><span class="line"></span><br><span class="line"><span class="attr">    mysql:</span></span><br><span class="line"><span class="attr">        container_name:</span> <span class="string">mysql</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">daocloud.io/library/mysql:8.0.20</span> <span class="comment"># 使用官方镜像</span></span><br><span class="line"><span class="attr">        ports:</span> </span><br><span class="line"><span class="bullet">            -</span> <span class="number">3307</span><span class="string">:3306</span> <span class="comment"># 本机端口:容器端口</span></span><br><span class="line"><span class="attr">        restart:</span> <span class="string">on-failure</span></span><br><span class="line"><span class="attr">        environment:</span> </span><br><span class="line"><span class="bullet">            -</span> <span class="string">MYSQL_ROOT_PASSWORD=123456</span> <span class="comment"># root用户密码</span></span><br><span class="line"><span class="attr">        volumes:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">./deploy/mysql/db:/var/lib/mysql</span> <span class="comment"># 用来存放了数据库表文件</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">./deploy/mysql/conf/my.cnf:/etc/my.cnf</span> <span class="comment"># 存放自定义的配置文件</span></span><br><span class="line">            <span class="comment"># 我们在启动MySQL容器时自动创建我们需要的数据库和表</span></span><br><span class="line">            <span class="comment"># mysql官方镜像中提供了容器启动时自动docker-entrypoint-initdb.d下的脚本的功能</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">./deploy/mysql/init:/docker-entrypoint-initdb.d/</span> <span class="comment"># 存放初始化的脚本</span></span><br><span class="line"><span class="attr">        networks:</span> </span><br><span class="line"><span class="bullet">            -</span> <span class="string">my-server</span></span><br><span class="line"></span><br><span class="line"><span class="attr">    server:</span> <span class="comment"># egg服务</span></span><br><span class="line"><span class="attr">        container_name:</span> <span class="string">server</span></span><br><span class="line"><span class="attr">        build:</span> <span class="comment"># 根据Dockerfile构建镜像</span></span><br><span class="line"><span class="attr">            context:</span> <span class="string">.</span></span><br><span class="line"><span class="attr">            dockerfile:</span> <span class="string">Dockerfile</span></span><br><span class="line"><span class="attr">        ports:</span> </span><br><span class="line"><span class="bullet">            -</span> <span class="number">7001</span><span class="string">:7001</span></span><br><span class="line"><span class="attr">        restart:</span> <span class="string">on-failure</span> <span class="comment"># 设置自动重启，这一步必须设置，主要是存在mysql还没有启动完成就启动了node服务</span></span><br><span class="line"><span class="attr">        networks:</span> </span><br><span class="line"><span class="bullet">            -</span> <span class="string">my-server</span></span><br><span class="line"><span class="attr">        depends_on:</span> <span class="comment"># node服务依赖于mysql和redis</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">redis</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">mysql</span></span><br><span class="line"></span><br><span class="line"><span class="attr">    nginx:</span></span><br><span class="line"><span class="attr">        container_name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">daocloud.io/library/nginx:1.13.0-alpine</span> <span class="comment"># 使用官方镜像</span></span><br><span class="line"><span class="attr">        ports:</span> </span><br><span class="line"><span class="bullet">            -</span> <span class="number">8900</span><span class="string">:80</span> <span class="comment"># 本地端口:容器端口</span></span><br><span class="line"><span class="attr">        restart:</span> <span class="string">on-failure</span></span><br><span class="line"><span class="attr">        volumes:</span> <span class="comment"># 映射本地目录到容器目录</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">./deploy/nginx/conf/nginx.conf:/etc/nginx/nginx.conf</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">./deploy/nginx/conf.d:/etc/nginx/conf.d</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">./deploy/nginx/html:/usr/share/nginx/html</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">./deploy/nginx/log:/var/log/nginx</span></span><br><span class="line"><span class="attr">        networks:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">my-server</span></span><br><span class="line"><span class="attr">        depends_on:</span> </span><br><span class="line"><span class="bullet">            -</span> <span class="string">redis</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">mysql</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">server</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明一下网桥  my-server。</span></span><br><span class="line"><span class="comment"># 重要：将所有服务都挂载在同一网桥即可通过容器名来互相通信了</span></span><br><span class="line"><span class="comment"># 如egg连接mysql和redis，可以通过容器名来互相通信</span></span><br><span class="line"><span class="attr">networks:</span></span><br><span class="line"><span class="attr">    my-server:</span></span><br></pre></td></tr></table></figure>
<h2 id="2-2-启动服务"><a href="#2-2-启动服务" class="headerlink" title="2.2 启动服务"></a>2.2 启动服务</h2><p><strong>修改egg服务代码</strong></p>
<p><img src="https://s.poetries.work/uploads/2022/06/a799a68a83398d4b.png" alt></p>
<p><strong>常用命令</strong></p>
<blockquote>
<p><code>docker-compose -h</code> 查看命令</p>
</blockquote>
<ul>
<li><code>docker-compose up</code> 启动服务，控制台可见日志</li>
<li><code>docker-compose up -d</code> 后台启动服务</li>
<li><code>docker-compose build --no-cache</code> 重新构建镜像不使用缓存(最后<code>docker-compose up -d</code>启动)</li>
<li>停止服务 <code>docker-compose down</code></li>
<li>下载镜像过程 <code>docker-compose pull</code></li>
<li>重启服务 <code>docker-compose restart</code></li>
</ul>
<p>后台启动服务 <code>docker-compose up -d</code></p>
<p><img src="https://s.poetries.work/uploads/2022/06/e8a4d267eb9342c1.png" alt></p>
<p>查看应用状态 <code>docker-compose ps</code></p>
<p><img src="https://s.poetries.work/uploads/2022/06/85c0f4122baf860d.png" alt></p>
<p>停止服务 <code>docker-compose down</code></p>
<h1 id="三、Nginx容器内部署前端"><a href="#三、Nginx容器内部署前端" class="headerlink" title="三、Nginx容器内部署前端"></a>三、Nginx容器内部署前端</h1><p>把前端打包的文件放到Nginx目录下访问</p>
<p><img src="https://s.poetries.work/uploads/2022/06/8ec0512b8a2a0652.png" alt></p>
<p><img src="https://s.poetries.work/uploads/2022/06/4a710e9a2a883e24.png" alt></p>
<p><img src="https://s.poetries.work/uploads/2022/06/0bb97a6d629301ac.png" alt></p>
<h1 id="四、docker部署到云服务器"><a href="#四、docker部署到云服务器" class="headerlink" title="四、docker部署到云服务器"></a>四、docker部署到云服务器</h1><h2 id="4-1-安装docker环境"><a href="#4-1-安装docker环境" class="headerlink" title="4.1 安装docker环境"></a>4.1 安装docker环境</h2><h3 id="安装工具包"><a href="#安装工具包" class="headerlink" title="安装工具包"></a>安装工具包</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install yum-utils device-mapper-persistent-data lvm2 -y</span><br></pre></td></tr></table></figure>
<p><img src="https://s.poetries.work/uploads/2022/06/e0f4f8f2621b11c0.png" alt></p>
<h3 id="设置阿里镜像源"><a href="#设置阿里镜像源" class="headerlink" title="设置阿里镜像源"></a>设置阿里镜像源</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure>
<p><img src="https://s.poetries.work/uploads/2022/06/0b017f9a76914c6e.png" alt></p>
<h3 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install docker-ce docker-ce-cli containerd.io -y</span><br></pre></td></tr></table></figure>
<h3 id="启动docker"><a href="#启动docker" class="headerlink" title="启动docker"></a>启动docker</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start docker</span><br><span class="line"></span><br><span class="line"># 设为开机启动</span><br><span class="line">systemctl enable docker</span><br></pre></td></tr></table></figure>
<h3 id="设置docker镜像源"><a href="#设置docker镜像源" class="headerlink" title="设置docker镜像源"></a>设置docker镜像源</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi /etc/docker/daemon.json</span><br></pre></td></tr></table></figure>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"registry-mirrors"</span>: [</span><br><span class="line">    <span class="string">"https://register.docker-cn.com/"</span></span><br><span class="line">  ],</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>后续拉取镜像直接从 <a href="https://hub.docker.com" target="_blank" rel="noopener">https://hub.docker.com</a> 网站拉取速度更快</p>
<p><strong>重启docker</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>
<h3 id="安装mysql镜像测试"><a href="#安装mysql镜像测试" class="headerlink" title="安装mysql镜像测试"></a>安装mysql镜像测试</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull daocloud.io/library/mysql:8.0.20</span><br></pre></td></tr></table></figure>
<p><img src="https://s.poetries.work/uploads/2022/06/dad88a1878a7a12f.png" alt></p>
<p><strong>运行mysql镜像</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -d -p 3307:3306 --name mysql -e MYSQL_ROOT_PASSWORD=123456(设置登录密码) be0dbf01a0f3(镜像ID)</span><br></pre></td></tr></table></figure>
<p><img src="https://s.poetries.work/uploads/2022/06/fb606d9823c4f0ff.png" alt></p>
<p><strong>进入mysql容器内部</strong></p>
<p><img src="https://s.poetries.work/uploads/2022/06/3a9f7618e1baf34f.png" alt></p>
<blockquote>
<p>至此mysql镜像搭建成功，下面我们使用<code>docker-compose</code>来管理docker容器，不在单独一个个安装MySQL、redis、nginx</p>
</blockquote>
<h2 id="4-2-安装docker-compose"><a href="#4-2-安装docker-compose" class="headerlink" title="4.2 安装docker-compose"></a>4.2 安装docker-compose</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 使用国内源安装</span><br><span class="line">curl -L https://get.daocloud.io/docker/compose/releases/download/1.22.0/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose</span><br></pre></td></tr></table></figure>
<p><strong>设置docker-compose执行权限</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod +x /usr/local/bin/docker-compose</span><br></pre></td></tr></table></figure>
<p><strong>创建软链</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose</span><br></pre></td></tr></table></figure>
<p><strong>测试是否安装成功：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker-compose --version</span><br><span class="line"></span><br><span class="line">docker-compose version 1.22.0, build f46880fe</span><br></pre></td></tr></table></figure>
<h2 id="4-3-开放服务器端口"><a href="#4-3-开放服务器端口" class="headerlink" title="4.3 开放服务器端口"></a>4.3 开放服务器端口</h2><p>登录服务器后台放行对应端口</p>
<p><img src="https://s.poetries.work/uploads/2022/06/e4657b28bcea9b61.png" alt></p>
<h2 id="4-4-部署egg项目"><a href="#4-4-部署egg项目" class="headerlink" title="4.4 部署egg项目"></a>4.4 部署egg项目</h2><h3 id="修改代码和配置"><a href="#修改代码和配置" class="headerlink" title="修改代码和配置"></a>修改代码和配置</h3><p><strong>修改Nginx配置</strong></p>
<p><img src="https://s.poetries.work/uploads/2022/06/bcaf72ea48991732.png" alt><br><img src="https://s.poetries.work/uploads/2022/06/1bf0445f94a779a6.png" alt></p>
<p><strong>修改config/config.prod.js</strong></p>
<p><img src="https://s.poetries.work/uploads/2022/06/2744890d4980cfc8.png" alt></p>
<p><strong>docker-compose.yml</strong></p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">"3.0"</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span> </span><br><span class="line">    <span class="comment"># docker容器启动的redis默认是没有redis.conf的配置文件，所以用docker启动redis之前，需要先去官网下载redis.conf的配置文件</span></span><br><span class="line"><span class="attr">    redis:</span> <span class="comment"># 服务名称</span></span><br><span class="line"><span class="attr">        container_name:</span> <span class="string">redis</span> <span class="comment"># 容器名称</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">daocloud.io/library/redis:6.0.3-alpine3.11</span> <span class="comment"># 使用官方镜像</span></span><br><span class="line"><span class="attr">        command:</span> <span class="string">redis-server</span> <span class="string">/usr/local/etc/redis/redis.conf</span> <span class="bullet">--requirepass</span> <span class="number">123456</span> <span class="bullet">--appendonly</span> <span class="literal">yes</span> <span class="comment"># 设置redis登录密码 123456、--appendonly yes：这个命令是用于开启redis数据持久化</span></span><br><span class="line">        <span class="comment"># command: redis-server --requirepass 123456 --appendonly yes # 设置redis登录密码 123456</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="number">6380</span><span class="string">:6379</span> <span class="comment"># 本机端口:容器端口</span></span><br><span class="line"><span class="attr">        restart:</span> <span class="string">on-failure</span> <span class="comment"># 自动重启</span></span><br><span class="line"><span class="attr">        volumes:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">./deploy/redis/db:/data</span> <span class="comment"># 把持久化数据挂载到宿主机</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">./deploy/redis/conf/redis.conf:/usr/local/etc/redis/redis.conf</span>  <span class="comment"># 把redis的配置文件挂载到宿主机</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">./deploy/redis/logs:/logs</span> <span class="comment"># 用来存放日志</span></span><br><span class="line"><span class="attr">        environment:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">TZ=Asia/Shanghai</span>  <span class="comment"># 解决容器 时区的问题</span></span><br><span class="line"><span class="attr">        networks:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">my-server</span></span><br><span class="line"></span><br><span class="line"><span class="attr">    mysql:</span></span><br><span class="line"><span class="attr">        container_name:</span> <span class="string">mysql</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">daocloud.io/library/mysql:8.0.20</span> <span class="comment"># 使用官方镜像</span></span><br><span class="line"><span class="attr">        ports:</span> </span><br><span class="line"><span class="bullet">            -</span> <span class="number">3307</span><span class="string">:3306</span> <span class="comment"># 本机端口:容器端口</span></span><br><span class="line"><span class="attr">        restart:</span> <span class="string">on-failure</span></span><br><span class="line"><span class="attr">        environment:</span> </span><br><span class="line"><span class="bullet">            -</span> <span class="string">MYSQL_ROOT_PASSWORD=993412</span> <span class="comment"># root用户密码</span></span><br><span class="line"><span class="attr">        volumes:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">./deploy/mysql/db:/var/lib/mysql</span> <span class="comment"># 用来存放了数据库表文件</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">./deploy/mysql/conf/my.cnf:/etc/my.cnf</span> <span class="comment"># 存放自定义的配置文件</span></span><br><span class="line">            <span class="comment"># 我们在启动MySQL容器时自动创建我们需要的数据库和表</span></span><br><span class="line">            <span class="comment"># mysql官方镜像中提供了容器启动时自动docker-entrypoint-initdb.d下的脚本的功能</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">./deploy/mysql/init:/docker-entrypoint-initdb.d/</span> <span class="comment"># 存放初始化的脚本</span></span><br><span class="line"><span class="attr">        networks:</span> </span><br><span class="line"><span class="bullet">            -</span> <span class="string">my-server</span></span><br><span class="line"></span><br><span class="line"><span class="attr">    server:</span> <span class="comment"># egg服务</span></span><br><span class="line"><span class="attr">        container_name:</span> <span class="string">server</span></span><br><span class="line"><span class="attr">        build:</span> <span class="comment"># 根据Dockerfile构建镜像</span></span><br><span class="line"><span class="attr">            context:</span> <span class="string">.</span></span><br><span class="line"><span class="attr">            dockerfile:</span> <span class="string">Dockerfile</span></span><br><span class="line"><span class="attr">        ports:</span> </span><br><span class="line"><span class="bullet">            -</span> <span class="number">7001</span><span class="string">:7001</span></span><br><span class="line"><span class="attr">        restart:</span> <span class="string">on-failure</span> <span class="comment"># 设置自动重启，这一步必须设置，主要是存在mysql还没有启动完成就启动了node服务</span></span><br><span class="line"><span class="attr">        networks:</span> </span><br><span class="line"><span class="bullet">            -</span> <span class="string">my-server</span></span><br><span class="line"><span class="attr">        depends_on:</span> <span class="comment"># node服务依赖于mysql和redis</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">redis</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">mysql</span></span><br><span class="line"></span><br><span class="line"><span class="attr">    nginx:</span></span><br><span class="line"><span class="attr">        container_name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">daocloud.io/library/nginx:1.13.0-alpine</span> <span class="comment"># 使用官方镜像</span></span><br><span class="line"><span class="attr">        ports:</span> </span><br><span class="line"><span class="bullet">            -</span> <span class="number">8900</span><span class="string">:80</span> <span class="comment"># 本地端口:容器端口</span></span><br><span class="line"><span class="attr">        restart:</span> <span class="string">on-failure</span></span><br><span class="line"><span class="attr">        volumes:</span> <span class="comment"># 映射本地目录到容器目录</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">./deploy/nginx/conf/nginx.conf:/etc/nginx/nginx.conf</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">./deploy/nginx/conf.d:/etc/nginx/conf.d</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">./deploy/nginx/html:/usr/share/nginx/html</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">./deploy/nginx/log:/var/log/nginx</span></span><br><span class="line"><span class="attr">        networks:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">my-server</span></span><br><span class="line"><span class="attr">        depends_on:</span> </span><br><span class="line"><span class="bullet">            -</span> <span class="string">redis</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">mysql</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">server</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明一下网桥  my-server。</span></span><br><span class="line"><span class="comment"># 重要：将所有服务都挂载在同一网桥即可通过容器名来互相通信了</span></span><br><span class="line"><span class="comment"># 如egg连接mysql和redis，可以通过容器名来互相通信</span></span><br><span class="line"><span class="attr">networks:</span></span><br><span class="line"><span class="attr">    my-server:</span></span><br></pre></td></tr></table></figure>
<p><strong>egg Dockerfile</strong></p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用node镜像</span></span><br><span class="line"><span class="string">FROM</span> <span class="string">daocloud.io/library/node:12.18</span></span><br><span class="line"><span class="comment"># 在容器中新建目录文件夹 egg</span></span><br><span class="line"><span class="string">RUN</span> <span class="string">mkdir</span> <span class="bullet">-p</span> <span class="string">/egg</span></span><br><span class="line"><span class="comment"># 将 /egg 设置为默认工作目录</span></span><br><span class="line"><span class="string">WORKDIR</span> <span class="string">/egg</span></span><br><span class="line"><span class="comment"># 将 package.json 复制默认工作目录</span></span><br><span class="line"><span class="string">COPY</span> <span class="string">package.json</span> <span class="string">/egg/package.json</span></span><br><span class="line"><span class="comment"># 安装依赖</span></span><br><span class="line"><span class="string">RUN</span> <span class="string">yarn</span> <span class="string">config</span> <span class="string">set</span> <span class="string">register</span> <span class="attr">https://registry.npm.taobao.org</span></span><br><span class="line"><span class="comment"># 只安装dependencies的包</span></span><br><span class="line"><span class="string">RUN</span> <span class="string">yarn</span> <span class="bullet">--production</span></span><br><span class="line"><span class="comment"># 再copy代码至容器</span></span><br><span class="line"><span class="string">COPY</span> <span class="string">./</span> <span class="string">/egg</span></span><br><span class="line"><span class="comment"># 7001端口</span></span><br><span class="line"><span class="string">EXPOSE</span> <span class="number">7001</span></span><br><span class="line"><span class="comment">#等容器启动之后执行脚本</span></span><br><span class="line"><span class="string">CMD</span> <span class="string">yarn</span> <span class="string">prod</span></span><br></pre></td></tr></table></figure>
<p><strong>./deploy/redis/conf/redis.conf</strong></p>
<p>需要设置的地方</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#指定日志级别，notice适用于生产环境</span><br><span class="line"># 指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose</span><br><span class="line"># debug (很多信息, 对开发／测试比较有用)</span><br><span class="line"># verbose (many rarely useful info, but not a mess like the debug level)</span><br><span class="line"># notice (moderately verbose, what you want in production probably)</span><br><span class="line"># warning (only very important / critical messages are logged)</span><br><span class="line">loglevel verbose</span><br><span class="line"></span><br><span class="line">#指定log日志位置</span><br><span class="line">logfile /logs/redis.log</span><br></pre></td></tr></table></figure>
<p>全部配置</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Redis configuration file example.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Note that in order to read the configuration file, Redis must be</span></span><br><span class="line"><span class="comment"># started with the file path as first argument:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># ./redis-server /path/to/redis.conf</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Note on units: when memory size is needed, it is possible to specify</span></span><br><span class="line"><span class="comment"># it in the usual form of 1k 5GB 4M and so forth:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 1k =&gt; 1000 bytes</span></span><br><span class="line"><span class="comment"># 1kb =&gt; 1024 bytes</span></span><br><span class="line"><span class="comment"># 1m =&gt; 1000000 bytes</span></span><br><span class="line"><span class="comment"># 1mb =&gt; 1024*1024 bytes</span></span><br><span class="line"><span class="comment"># 1g =&gt; 1000000000 bytes</span></span><br><span class="line"><span class="comment"># 1gb =&gt; 1024*1024*1024 bytes</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># units are case insensitive so 1GB 1Gb 1gB are all the same.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################## INCLUDES ###################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Include one or more other config files here.  This is useful if you</span></span><br><span class="line"><span class="comment"># have a standard template that goes to all Redis servers but also need</span></span><br><span class="line"><span class="comment"># to customize a few per-server settings.  Include files can include</span></span><br><span class="line"><span class="comment"># other files, so use this wisely.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Notice option "include" won't be rewritten by command "CONFIG REWRITE"</span></span><br><span class="line"><span class="comment"># from admin or Redis Sentinel. Since Redis always uses the last processed</span></span><br><span class="line"><span class="comment"># line as value of a configuration directive, you'd better put includes</span></span><br><span class="line"><span class="comment"># at the beginning of this file to avoid overwriting config change at runtime.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># If instead you are interested in using includes to override configuration</span></span><br><span class="line"><span class="comment"># options, it is better to use include as the last line.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># include /path/to/local.conf</span></span><br><span class="line"><span class="comment"># include /path/to/other.conf</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################## MODULES #####################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load modules at startup. If the server is not able to load modules</span></span><br><span class="line"><span class="comment"># it will abort. It is possible to use multiple loadmodule directives.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># loadmodule /path/to/my_module.so</span></span><br><span class="line"><span class="comment"># loadmodule /path/to/other_module.so</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################## NETWORK #####################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># By default, if no "bind" configuration directive is specified, Redis listens</span></span><br><span class="line"><span class="comment"># for connections from all the network interfaces available on the server.</span></span><br><span class="line"><span class="comment"># It is possible to listen to just one or multiple selected interfaces using</span></span><br><span class="line"><span class="comment"># the "bind" configuration directive, followed by one or more IP addresses.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Examples:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># bind 192.168.1.100 10.0.0.1</span></span><br><span class="line"><span class="comment"># bind 127.0.0.1 ::1</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the</span></span><br><span class="line"><span class="comment"># internet, binding to all the interfaces is dangerous and will expose the</span></span><br><span class="line"><span class="comment"># instance to everybody on the internet. So by default we uncomment the</span></span><br><span class="line"><span class="comment"># following bind directive, that will force Redis to listen only into</span></span><br><span class="line"><span class="comment"># the IPv4 loopback interface address (this means Redis will be able to</span></span><br><span class="line"><span class="comment"># accept connections only from clients running into the same computer it</span></span><br><span class="line"><span class="comment"># is running).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES</span></span><br><span class="line"><span class="comment"># JUST COMMENT THE FOLLOWING LINE.</span></span><br><span class="line"><span class="comment"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span></span><br><span class="line"><span class="comment"># redis绑定的ip或者主机名，注意如果此处绑定设置为127.0.0.1，将会出现其他服务器上的服务连接至此台redis失败的情况</span></span><br><span class="line"><span class="comment"># 绑定的主机地址</span></span><br><span class="line"><span class="comment"># 你可以绑定单一接口，如果没有绑定，所有接口都会监听到来的连接</span></span><br><span class="line"><span class="comment"># bind 127.0.0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Protected mode is a layer of security protection, in order to avoid that</span></span><br><span class="line"><span class="comment"># Redis instances left open on the internet are accessed and exploited.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># When protected mode is on and if:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 1) The server is not binding explicitly to a set of addresses using the</span></span><br><span class="line"><span class="comment">#    "bind" directive.</span></span><br><span class="line"><span class="comment"># 2) No password is configured.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The server only accepts connections from clients connecting from the</span></span><br><span class="line"><span class="comment"># IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain</span></span><br><span class="line"><span class="comment"># sockets.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># By default protected mode is enabled. You should disable it only if</span></span><br><span class="line"><span class="comment"># you are sure you want clients from other hosts to connect to Redis</span></span><br><span class="line"><span class="comment"># even if no authentication is configured, nor a specific set of interfaces</span></span><br><span class="line"><span class="comment"># are explicitly listed using the "bind" directive.</span></span><br><span class="line"><span class="string">protected-mode</span> <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Accept connections on the specified port, default is 6379 (IANA #815344).</span></span><br><span class="line"><span class="comment"># If port 0 is specified Redis will not listen on a TCP socket.</span></span><br><span class="line"><span class="comment"># 指定redis启动占用的端口</span></span><br><span class="line"><span class="string">port</span> <span class="number">6379</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># TCP listen() backlog.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># In high requests-per-second environments you need an high backlog in order</span></span><br><span class="line"><span class="comment"># to avoid slow clients connections issues. Note that the Linux kernel</span></span><br><span class="line"><span class="comment"># will silently truncate it to the value of /proc/sys/net/core/somaxconn so</span></span><br><span class="line"><span class="comment"># make sure to raise both the value of somaxconn and tcp_max_syn_backlog</span></span><br><span class="line"><span class="comment"># in order to get the desired effect.</span></span><br><span class="line"><span class="comment">#此项配置内容属于redis优化内容</span></span><br><span class="line"><span class="string">tcp-backlog</span> <span class="number">511</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Unix socket.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Specify the path for the Unix socket that will be used to listen for</span></span><br><span class="line"><span class="comment"># incoming connections. There is no default, so Redis will not listen</span></span><br><span class="line"><span class="comment"># on a unix socket when not specified.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># unixsocket /tmp/redis.sock</span></span><br><span class="line"><span class="comment"># unixsocketperm 700</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Close the connection after a client is idle for N seconds (0 to disable)</span></span><br><span class="line"><span class="comment"># 指定socket连接空闲时间（秒），如果连接空闲超时将会关闭连接，设置为0表示用不超时</span></span><br><span class="line"><span class="string">timeout</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># TCP keepalive.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence</span></span><br><span class="line"><span class="comment"># of communication. This is useful for two reasons:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 1) Detect dead peers.</span></span><br><span class="line"><span class="comment"># 2) Take the connection alive from the point of view of network</span></span><br><span class="line"><span class="comment">#    equipment in the middle.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># On Linux, the specified value (in seconds) is the period used to send ACKs.</span></span><br><span class="line"><span class="comment"># Note that to close the connection the double of the time is needed.</span></span><br><span class="line"><span class="comment"># On other kernels the period depends on the kernel configuration.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># A reasonable value for this option is 300 seconds, which is the new</span></span><br><span class="line"><span class="comment"># Redis default starting with Redis 3.2.1.</span></span><br><span class="line"><span class="comment">#指定tcp连接是否为长连接，长连接将会额外增加server端的开支，默认为0表示禁用</span></span><br><span class="line"><span class="string">tcp-keepalive</span> <span class="number">300</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################# TLS/SSL #####################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># By default, TLS/SSL is disabled. To enable it, the "tls-port" configuration</span></span><br><span class="line"><span class="comment"># directive can be used to define TLS-listening ports. To enable TLS on the</span></span><br><span class="line"><span class="comment"># default port, use:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># port 0</span></span><br><span class="line"><span class="comment"># tls-port 6379</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure a X.509 certificate and private key to use for authenticating the</span></span><br><span class="line"><span class="comment"># server to connected clients, masters or cluster peers.  These files should be</span></span><br><span class="line"><span class="comment"># PEM formatted.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># tls-cert-file redis.crt </span></span><br><span class="line"><span class="comment"># tls-key-file redis.key</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure a DH parameters file to enable Diffie-Hellman (DH) key exchange:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># tls-dh-params-file redis.dh</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure a CA certificate(s) bundle or directory to authenticate TLS/SSL</span></span><br><span class="line"><span class="comment"># clients and peers.  Redis requires an explicit configuration of at least one</span></span><br><span class="line"><span class="comment"># of these, and will not implicitly use the system wide configuration.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># tls-ca-cert-file ca.crt</span></span><br><span class="line"><span class="comment"># tls-ca-cert-dir /etc/ssl/certs</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># By default, clients (including replica servers) on a TLS port are required</span></span><br><span class="line"><span class="comment"># to authenticate using valid client side certificates.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># It is possible to disable authentication using this directive.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># tls-auth-clients no</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># By default, a Redis replica does not attempt to establish a TLS connection</span></span><br><span class="line"><span class="comment"># with its master.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Use the following directive to enable TLS on replication links.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># tls-replication yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># By default, the Redis Cluster bus uses a plain TCP connection. To enable</span></span><br><span class="line"><span class="comment"># TLS for the bus protocol, use the following directive:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># tls-cluster yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Explicitly specify TLS versions to support. Allowed values are case insensitive</span></span><br><span class="line"><span class="comment"># and include "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3" (OpenSSL &gt;= 1.1.1) or</span></span><br><span class="line"><span class="comment"># any combination. To enable only TLSv1.2 and TLSv1.3, use:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># tls-protocols "TLSv1.2 TLSv1.3"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure allowed ciphers.  See the ciphers(1ssl) manpage for more information</span></span><br><span class="line"><span class="comment"># about the syntax of this string.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Note: this configuration applies only to &lt;= TLSv1.2.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># tls-ciphers DEFAULT:!MEDIUM</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure allowed TLSv1.3 ciphersuites.  See the ciphers(1ssl) manpage for more</span></span><br><span class="line"><span class="comment"># information about the syntax of this string, and specifically for TLSv1.3</span></span><br><span class="line"><span class="comment"># ciphersuites.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># tls-ciphersuites TLS_CHACHA20_POLY1305_SHA256</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># When choosing a cipher, use the server's preference instead of the client</span></span><br><span class="line"><span class="comment"># preference. By default, the server follows the client's preference.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># tls-prefer-server-ciphers yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################# GENERAL #####################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># By default Redis does not run as a daemon. Use 'yes' if you need it.</span></span><br><span class="line"><span class="comment"># Note that Redis will write a pid file in /var/run/redis.pid when daemonized.</span></span><br><span class="line"><span class="comment"># 默认以后台方式运行 yes 则以后台方式运行</span></span><br><span class="line"><span class="string">daemonize</span> <span class="literal">no</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># If you run Redis from upstart or systemd, Redis can interact with your</span></span><br><span class="line"><span class="comment"># supervision tree. Options:</span></span><br><span class="line"><span class="comment">#   supervised no      - no supervision interaction</span></span><br><span class="line"><span class="comment">#   supervised upstart - signal upstart by putting Redis into SIGSTOP mode</span></span><br><span class="line"><span class="comment">#   supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET</span></span><br><span class="line"><span class="comment">#   supervised auto    - detect upstart or systemd method based on</span></span><br><span class="line"><span class="comment">#                        UPSTART_JOB or NOTIFY_SOCKET environment variables</span></span><br><span class="line"><span class="comment"># Note: these supervision methods only signal "process is ready."</span></span><br><span class="line"><span class="comment">#       They do not enable continuous liveness pings back to your supervisor.</span></span><br><span class="line"><span class="string">supervised</span> <span class="literal">no</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># If a pid file is specified, Redis writes it where specified at startup</span></span><br><span class="line"><span class="comment"># and removes it at exit.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># When the server runs non daemonized, no pid file is created if none is</span></span><br><span class="line"><span class="comment"># specified in the configuration. When the server is daemonized, the pid file</span></span><br><span class="line"><span class="comment"># is used even if not specified, defaulting to "/var/run/redis.pid".</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Creating a pid file is best effort: if Redis is not able to create it</span></span><br><span class="line"><span class="comment"># nothing bad happens, the server will start and run normally.</span></span><br><span class="line"><span class="comment"># 指定redis pid文件</span></span><br><span class="line"><span class="string">pidfile</span> <span class="string">/var/run/redis_6379.pid</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify the server verbosity level.</span></span><br><span class="line"><span class="comment"># This can be one of:</span></span><br><span class="line"><span class="comment"># debug (a lot of information, useful for development/testing)</span></span><br><span class="line"><span class="comment"># verbose (many rarely useful info, but not a mess like the debug level)</span></span><br><span class="line"><span class="comment"># notice (moderately verbose, what you want in production probably)</span></span><br><span class="line"><span class="comment"># warning (only very important / critical messages are logged)</span></span><br><span class="line"><span class="comment">#指定日志级别，notice适用于生产环境</span></span><br><span class="line"><span class="comment"># 指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose</span></span><br><span class="line"><span class="comment"># debug (很多信息, 对开发／测试比较有用)</span></span><br><span class="line"><span class="comment"># verbose (many rarely useful info, but not a mess like the debug level)</span></span><br><span class="line"><span class="comment"># notice (moderately verbose, what you want in production probably)</span></span><br><span class="line"><span class="comment"># warning (only very important / critical messages are logged)</span></span><br><span class="line"><span class="string">loglevel</span> <span class="string">verbose</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify the log file name. Also the empty string can be used to force</span></span><br><span class="line"><span class="comment"># Redis to log on the standard output. Note that if you use standard</span></span><br><span class="line"><span class="comment"># output for logging but daemonize, logs will be sent to /dev/null</span></span><br><span class="line"><span class="comment">#指定log日志位置</span></span><br><span class="line"><span class="string">logfile</span> <span class="string">/logs/redis.log</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># To enable logging to the system logger, just set 'syslog-enabled' to yes,</span></span><br><span class="line"><span class="comment"># and optionally update the other syslog parameters to suit your needs.</span></span><br><span class="line"><span class="comment">#是否将日志输出到系统日志，默认为no</span></span><br><span class="line"><span class="string">syslog-enabled</span> <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify the syslog identity.</span></span><br><span class="line"><span class="comment">#指定syslog的标示符，如果'syslog-enabled'是no，则这个选项无效</span></span><br><span class="line"><span class="string">syslog-ident</span> <span class="string">redis</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.</span></span><br><span class="line"><span class="comment"># syslog-facility local0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the number of databases. The default database is DB 0, you can select</span></span><br><span class="line"><span class="comment"># a different one on a per-connection basis using SELECT &lt;dbid&gt; where</span></span><br><span class="line"><span class="comment"># dbid is a number between 0 and 'databases'-1</span></span><br><span class="line"><span class="comment"># 设定redis所允许的最大"db簇"的个数,默认为16个簇</span></span><br><span class="line"><span class="string">databases</span> <span class="number">16</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># By default Redis shows an ASCII art logo only when started to log to the</span></span><br><span class="line"><span class="comment"># standard output and if the standard output is a TTY. Basically this means</span></span><br><span class="line"><span class="comment"># that normally a logo is displayed only in interactive sessions.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># However it is possible to force the pre-4.0 behavior and always show a</span></span><br><span class="line"><span class="comment"># ASCII art logo in startup logs by setting the following option to yes.</span></span><br><span class="line"><span class="string">always-show-logo</span> <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################ SNAPSHOTTING  ################################</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Save the DB on disk:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   save &lt;seconds&gt; &lt;changes&gt;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   Will save the DB if both the given number of seconds and the given</span></span><br><span class="line"><span class="comment">#   number of write operations against the DB occurred.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   In the example below the behaviour will be to save:</span></span><br><span class="line"><span class="comment">#   after 900 sec (15 min) if at least 1 key changed</span></span><br><span class="line"><span class="comment">#   after 300 sec (5 min) if at least 10 keys changed</span></span><br><span class="line"><span class="comment">#   after 60 sec if at least 10000 keys changed</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   Note: you can disable saving completely by commenting out all "save" lines.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   It is also possible to remove all the previously configured save</span></span><br><span class="line"><span class="comment">#   points by adding a save directive with a single empty string argument</span></span><br><span class="line"><span class="comment">#   like in the following example:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   save ""</span></span><br><span class="line"></span><br><span class="line"><span class="string">save</span> <span class="number">900</span> <span class="number">1</span></span><br><span class="line"><span class="string">save</span> <span class="number">300</span> <span class="number">10</span></span><br><span class="line"><span class="string">save</span> <span class="number">60</span> <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># By default Redis will stop accepting writes if RDB snapshots are enabled</span></span><br><span class="line"><span class="comment"># (at least one save point) and the latest background save failed.</span></span><br><span class="line"><span class="comment"># This will make the user aware (in a hard way) that data is not persisting</span></span><br><span class="line"><span class="comment"># on disk properly, otherwise chances are that no one will notice and some</span></span><br><span class="line"><span class="comment"># disaster will happen.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># If the background saving process will start working again Redis will</span></span><br><span class="line"><span class="comment"># automatically allow writes again.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># However if you have setup your proper monitoring of the Redis server</span></span><br><span class="line"><span class="comment"># and persistence, you may want to disable this feature so that Redis will</span></span><br><span class="line"><span class="comment"># continue to work as usual even if there are problems with disk,</span></span><br><span class="line"><span class="comment"># permissions, and so forth.</span></span><br><span class="line"><span class="comment">#如果snapshot过程中出现错误,即数据持久化失败,是否终止所有的客户端write请求</span></span><br><span class="line"><span class="string">stop-writes-on-bgsave-error</span> <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compress string objects using LZF when dump .rdb databases?</span></span><br><span class="line"><span class="comment"># For default that's set to 'yes' as it's almost always a win.</span></span><br><span class="line"><span class="comment"># If you want to save some CPU in the saving child set it to 'no' but</span></span><br><span class="line"><span class="comment"># the dataset will likely be bigger if you have compressible values or keys.</span></span><br><span class="line"><span class="comment">#是否启用rdb文件压缩手段,默认为yes</span></span><br><span class="line"><span class="string">rdbcompression</span> <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Since version 5 of RDB a CRC64 checksum is placed at the end of the file.</span></span><br><span class="line"><span class="comment"># This makes the format more resistant to corruption but there is a performance</span></span><br><span class="line"><span class="comment"># hit to pay (around 10%) when saving and loading RDB files, so you can disable it</span></span><br><span class="line"><span class="comment"># for maximum performances.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># RDB files created with checksum disabled have a checksum of zero that will</span></span><br><span class="line"><span class="comment"># tell the loading code to skip the check.</span></span><br><span class="line"><span class="comment"># 是否对rdb文件使用CRC64校验和,默认为"yes",那么每个rdb文件内容的末尾都会追加CRC校验和</span></span><br><span class="line"><span class="string">rdbchecksum</span> <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The filename where to dump the DB</span></span><br><span class="line"><span class="comment">#指定rdb文件的名称</span></span><br><span class="line"><span class="string">dbfilename</span> <span class="string">dump.rdb</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Remove RDB files used by replication in instances without persistence</span></span><br><span class="line"><span class="comment"># enabled. By default this option is disabled, however there are environments</span></span><br><span class="line"><span class="comment"># where for regulations or other security concerns, RDB files persisted on</span></span><br><span class="line"><span class="comment"># disk by masters in order to feed replicas, or stored on disk by replicas</span></span><br><span class="line"><span class="comment"># in order to load them for the initial synchronization, should be deleted</span></span><br><span class="line"><span class="comment"># ASAP. Note that this option ONLY WORKS in instances that have both AOF</span></span><br><span class="line"><span class="comment"># and RDB persistence disabled, otherwise is completely ignored.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># An alternative (and sometimes better) way to obtain the same effect is</span></span><br><span class="line"><span class="comment"># to use diskless replication on both master and replicas instances. However</span></span><br><span class="line"><span class="comment"># in the case of replicas, diskless is not always an option.</span></span><br><span class="line"><span class="string">rdb-del-sync-files</span> <span class="literal">no</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The working directory.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The DB will be written inside this directory, with the filename specified</span></span><br><span class="line"><span class="comment"># above using the 'dbfilename' configuration directive.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The Append Only File will also be created inside this directory.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Note that you must specify a directory here, not a file name.</span></span><br><span class="line"><span class="comment">#指定rdb/AOF文件的目录位置</span></span><br><span class="line"><span class="string">dir</span> <span class="string">./</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################# REPLICATION #################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Master-Replica replication. Use replicaof to make a Redis instance a copy of</span></span><br><span class="line"><span class="comment"># another Redis server. A few things to understand ASAP about Redis replication.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   +------------------+      +---------------+</span></span><br><span class="line"><span class="comment">#   |      Master      | ---&gt; |    Replica    |</span></span><br><span class="line"><span class="comment">#   | (receive writes) |      |  (exact copy) |</span></span><br><span class="line"><span class="comment">#   +------------------+      +---------------+</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 1) Redis replication is asynchronous, but you can configure a master to</span></span><br><span class="line"><span class="comment">#    stop accepting writes if it appears to be not connected with at least</span></span><br><span class="line"><span class="comment">#    a given number of replicas.</span></span><br><span class="line"><span class="comment"># 2) Redis replicas are able to perform a partial resynchronization with the</span></span><br><span class="line"><span class="comment">#    master if the replication link is lost for a relatively small amount of</span></span><br><span class="line"><span class="comment">#    time. You may want to configure the replication backlog size (see the next</span></span><br><span class="line"><span class="comment">#    sections of this file) with a sensible value depending on your needs.</span></span><br><span class="line"><span class="comment"># 3) Replication is automatic and does not need user intervention. After a</span></span><br><span class="line"><span class="comment">#    network partition replicas automatically try to reconnect to masters</span></span><br><span class="line"><span class="comment">#    and resynchronize with them.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># replicaof &lt;masterip&gt; &lt;masterport&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># If the master is password protected (using the "requirepass" configuration</span></span><br><span class="line"><span class="comment"># directive below) it is possible to tell the replica to authenticate before</span></span><br><span class="line"><span class="comment"># starting the replication synchronization process, otherwise the master will</span></span><br><span class="line"><span class="comment"># refuse the replica request.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># masterauth &lt;master-password&gt;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># However this is not enough if you are using Redis ACLs (for Redis version</span></span><br><span class="line"><span class="comment"># 6 or greater), and the default user is not capable of running the PSYNC</span></span><br><span class="line"><span class="comment"># command and/or other commands needed for replication. In this case it's</span></span><br><span class="line"><span class="comment"># better to configure a special user to use with replication, and specify the</span></span><br><span class="line"><span class="comment"># masteruser configuration as such:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># masteruser &lt;username&gt;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># When masteruser is specified, the replica will authenticate against its</span></span><br><span class="line"><span class="comment"># master using the new AUTH form: AUTH &lt;username&gt; &lt;password&gt;.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># When a replica loses its connection with the master, or when the replication</span></span><br><span class="line"><span class="comment"># is still in progress, the replica can act in two different ways:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 1) if replica-serve-stale-data is set to 'yes' (the default) the replica will</span></span><br><span class="line"><span class="comment">#    still reply to client requests, possibly with out of date data, or the</span></span><br><span class="line"><span class="comment">#    data set may just be empty if this is the first synchronization.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 2) if replica-serve-stale-data is set to 'no' the replica will reply with</span></span><br><span class="line"><span class="comment">#    an error "SYNC with master in progress" to all the kind of commands</span></span><br><span class="line"><span class="comment">#    but to INFO, replicaOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG,</span></span><br><span class="line"><span class="comment">#    SUBSCRIBE, UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB,</span></span><br><span class="line"><span class="comment">#    COMMAND, POST, HOST: and LATENCY.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="string">replica-serve-stale-data</span> <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># You can configure a replica instance to accept writes or not. Writing against</span></span><br><span class="line"><span class="comment"># a replica instance may be useful to store some ephemeral data (because data</span></span><br><span class="line"><span class="comment"># written on a replica will be easily deleted after resync with the master) but</span></span><br><span class="line"><span class="comment"># may also cause problems if clients are writing to it because of a</span></span><br><span class="line"><span class="comment"># misconfiguration.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Since Redis 2.6 by default replicas are read-only.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Note: read only replicas are not designed to be exposed to untrusted clients</span></span><br><span class="line"><span class="comment"># on the internet. It's just a protection layer against misuse of the instance.</span></span><br><span class="line"><span class="comment"># Still a read only replica exports by default all the administrative commands</span></span><br><span class="line"><span class="comment"># such as CONFIG, DEBUG, and so forth. To a limited extent you can improve</span></span><br><span class="line"><span class="comment"># security of read only replicas using 'rename-command' to shadow all the</span></span><br><span class="line"><span class="comment"># administrative / dangerous commands.</span></span><br><span class="line"><span class="string">replica-read-only</span> <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Replication SYNC strategy: disk or socket.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># New replicas and reconnecting replicas that are not able to continue the</span></span><br><span class="line"><span class="comment"># replication process just receiving differences, need to do what is called a</span></span><br><span class="line"><span class="comment"># "full synchronization". An RDB file is transmitted from the master to the</span></span><br><span class="line"><span class="comment"># replicas.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The transmission can happen in two different ways:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 1) Disk-backed: The Redis master creates a new process that writes the RDB</span></span><br><span class="line"><span class="comment">#                 file on disk. Later the file is transferred by the parent</span></span><br><span class="line"><span class="comment">#                 process to the replicas incrementally.</span></span><br><span class="line"><span class="comment"># 2) Diskless: The Redis master creates a new process that directly writes the</span></span><br><span class="line"><span class="comment">#              RDB file to replica sockets, without touching the disk at all.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># With disk-backed replication, while the RDB file is generated, more replicas</span></span><br><span class="line"><span class="comment"># can be queued and served with the RDB file as soon as the current child</span></span><br><span class="line"><span class="comment"># producing the RDB file finishes its work. With diskless replication instead</span></span><br><span class="line"><span class="comment"># once the transfer starts, new replicas arriving will be queued and a new</span></span><br><span class="line"><span class="comment"># transfer will start when the current one terminates.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># When diskless replication is used, the master waits a configurable amount of</span></span><br><span class="line"><span class="comment"># time (in seconds) before starting the transfer in the hope that multiple</span></span><br><span class="line"><span class="comment"># replicas will arrive and the transfer can be parallelized.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># With slow disks and fast (large bandwidth) networks, diskless replication</span></span><br><span class="line"><span class="comment"># works better.</span></span><br><span class="line"><span class="string">repl-diskless-sync</span> <span class="literal">no</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># When diskless replication is enabled, it is possible to configure the delay</span></span><br><span class="line"><span class="comment"># the server waits in order to spawn the child that transfers the RDB via socket</span></span><br><span class="line"><span class="comment"># to the replicas.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># This is important since once the transfer starts, it is not possible to serve</span></span><br><span class="line"><span class="comment"># new replicas arriving, that will be queued for the next RDB transfer, so the</span></span><br><span class="line"><span class="comment"># server waits a delay in order to let more replicas arrive.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The delay is specified in seconds, and by default is 5 seconds. To disable</span></span><br><span class="line"><span class="comment"># it entirely just set it to 0 seconds and the transfer will start ASAP.</span></span><br><span class="line"><span class="string">repl-diskless-sync-delay</span> <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># WARNING: RDB diskless load is experimental. Since in this setup the replica</span></span><br><span class="line"><span class="comment"># does not immediately store an RDB on disk, it may cause data loss during</span></span><br><span class="line"><span class="comment"># failovers. RDB diskless load + Redis modules not handling I/O reads may also</span></span><br><span class="line"><span class="comment"># cause Redis to abort in case of I/O errors during the initial synchronization</span></span><br><span class="line"><span class="comment"># stage with the master. Use only if your do what you are doing.</span></span><br><span class="line"><span class="comment"># -----------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Replica can load the RDB it reads from the replication link directly from the</span></span><br><span class="line"><span class="comment"># socket, or store the RDB to a file and read that file after it was completely</span></span><br><span class="line"><span class="comment"># recived from the master.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># In many cases the disk is slower than the network, and storing and loading</span></span><br><span class="line"><span class="comment"># the RDB file may increase replication time (and even increase the master's</span></span><br><span class="line"><span class="comment"># Copy on Write memory and salve buffers).</span></span><br><span class="line"><span class="comment"># However, parsing the RDB file directly from the socket may mean that we have</span></span><br><span class="line"><span class="comment"># to flush the contents of the current database before the full rdb was</span></span><br><span class="line"><span class="comment"># received. For this reason we have the following options:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># "disabled"    - Don't use diskless load (store the rdb file to the disk first)</span></span><br><span class="line"><span class="comment"># "on-empty-db" - Use diskless load only when it is completely safe.</span></span><br><span class="line"><span class="comment"># "swapdb"      - Keep a copy of the current db contents in RAM while parsing</span></span><br><span class="line"><span class="comment">#                 the data directly from the socket. note that this requires</span></span><br><span class="line"><span class="comment">#                 sufficient memory, if you don't have it, you risk an OOM kill.</span></span><br><span class="line"><span class="string">repl-diskless-load</span> <span class="string">disabled</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Replicas send PINGs to server in a predefined interval. It's possible to</span></span><br><span class="line"><span class="comment"># change this interval with the repl_ping_replica_period option. The default</span></span><br><span class="line"><span class="comment"># value is 10 seconds.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># repl-ping-replica-period 10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The following option sets the replication timeout for:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 1) Bulk transfer I/O during SYNC, from the point of view of replica.</span></span><br><span class="line"><span class="comment"># 2) Master timeout from the point of view of replicas (data, pings).</span></span><br><span class="line"><span class="comment"># 3) Replica timeout from the point of view of masters (REPLCONF ACK pings).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># It is important to make sure that this value is greater than the value</span></span><br><span class="line"><span class="comment"># specified for repl-ping-replica-period otherwise a timeout will be detected</span></span><br><span class="line"><span class="comment"># every time there is low traffic between the master and the replica.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># repl-timeout 60</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Disable TCP_NODELAY on the replica socket after SYNC?</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># If you select "yes" Redis will use a smaller number of TCP packets and</span></span><br><span class="line"><span class="comment"># less bandwidth to send data to replicas. But this can add a delay for</span></span><br><span class="line"><span class="comment"># the data to appear on the replica side, up to 40 milliseconds with</span></span><br><span class="line"><span class="comment"># Linux kernels using a default configuration.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># If you select "no" the delay for data to appear on the replica side will</span></span><br><span class="line"><span class="comment"># be reduced but more bandwidth will be used for replication.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># By default we optimize for low latency, but in very high traffic conditions</span></span><br><span class="line"><span class="comment"># or when the master and replicas are many hops away, turning this to "yes" may</span></span><br><span class="line"><span class="comment"># be a good idea.</span></span><br><span class="line"><span class="string">repl-disable-tcp-nodelay</span> <span class="literal">no</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the replication backlog size. The backlog is a buffer that accumulates</span></span><br><span class="line"><span class="comment"># replica data when replicas are disconnected for some time, so that when a</span></span><br><span class="line"><span class="comment"># replica wants to reconnect again, often a full resync is not needed, but a</span></span><br><span class="line"><span class="comment"># partial resync is enough, just passing the portion of data the replica</span></span><br><span class="line"><span class="comment"># missed while disconnected.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The bigger the replication backlog, the longer the time the replica can be</span></span><br><span class="line"><span class="comment"># disconnected and later be able to perform a partial resynchronization.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The backlog is only allocated once there is at least a replica connected.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># repl-backlog-size 1mb</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># After a master has no longer connected replicas for some time, the backlog</span></span><br><span class="line"><span class="comment"># will be freed. The following option configures the amount of seconds that</span></span><br><span class="line"><span class="comment"># need to elapse, starting from the time the last replica disconnected, for</span></span><br><span class="line"><span class="comment"># the backlog buffer to be freed.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Note that replicas never free the backlog for timeout, since they may be</span></span><br><span class="line"><span class="comment"># promoted to masters later, and should be able to correctly "partially</span></span><br><span class="line"><span class="comment"># resynchronize" with the replicas: hence they should always accumulate backlog.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># A value of 0 means to never release the backlog.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># repl-backlog-ttl 3600</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The replica priority is an integer number published by Redis in the INFO</span></span><br><span class="line"><span class="comment"># output. It is used by Redis Sentinel in order to select a replica to promote</span></span><br><span class="line"><span class="comment"># into a master if the master is no longer working correctly.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># A replica with a low priority number is considered better for promotion, so</span></span><br><span class="line"><span class="comment"># for instance if there are three replicas with priority 10, 100, 25 Sentinel</span></span><br><span class="line"><span class="comment"># will pick the one with priority 10, that is the lowest.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># However a special priority of 0 marks the replica as not able to perform the</span></span><br><span class="line"><span class="comment"># role of master, so a replica with priority of 0 will never be selected by</span></span><br><span class="line"><span class="comment"># Redis Sentinel for promotion.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># By default the priority is 100.</span></span><br><span class="line"><span class="string">replica-priority</span> <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># It is possible for a master to stop accepting writes if there are less than</span></span><br><span class="line"><span class="comment"># N replicas connected, having a lag less or equal than M seconds.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The N replicas need to be in "online" state.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The lag in seconds, that must be &lt;= the specified value, is calculated from</span></span><br><span class="line"><span class="comment"># the last ping received from the replica, that is usually sent every second.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># This option does not GUARANTEE that N replicas will accept the write, but</span></span><br><span class="line"><span class="comment"># will limit the window of exposure for lost writes in case not enough replicas</span></span><br><span class="line"><span class="comment"># are available, to the specified number of seconds.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># For example to require at least 3 replicas with a lag &lt;= 10 seconds use:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># min-replicas-to-write 3</span></span><br><span class="line"><span class="comment"># min-replicas-max-lag 10</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Setting one or the other to 0 disables the feature.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># By default min-replicas-to-write is set to 0 (feature disabled) and</span></span><br><span class="line"><span class="comment"># min-replicas-max-lag is set to 10.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A Redis master is able to list the address and port of the attached</span></span><br><span class="line"><span class="comment"># replicas in different ways. For example the "INFO replication" section</span></span><br><span class="line"><span class="comment"># offers this information, which is used, among other tools, by</span></span><br><span class="line"><span class="comment"># Redis Sentinel in order to discover replica instances.</span></span><br><span class="line"><span class="comment"># Another place where this info is available is in the output of the</span></span><br><span class="line"><span class="comment"># "ROLE" command of a master.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The listed IP and address normally reported by a replica is obtained</span></span><br><span class="line"><span class="comment"># in the following way:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   IP: The address is auto detected by checking the peer address</span></span><br><span class="line"><span class="comment">#   of the socket used by the replica to connect with the master.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   Port: The port is communicated by the replica during the replication</span></span><br><span class="line"><span class="comment">#   handshake, and is normally the port that the replica is using to</span></span><br><span class="line"><span class="comment">#   listen for connections.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># However when port forwarding or Network Address Translation (NAT) is</span></span><br><span class="line"><span class="comment"># used, the replica may be actually reachable via different IP and port</span></span><br><span class="line"><span class="comment"># pairs. The following two options can be used by a replica in order to</span></span><br><span class="line"><span class="comment"># report to its master a specific set of IP and port, so that both INFO</span></span><br><span class="line"><span class="comment"># and ROLE will report those values.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># There is no need to use both the options if you need to override just</span></span><br><span class="line"><span class="comment"># the port or the IP address.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># replica-announce-ip 5.5.5.5</span></span><br><span class="line"><span class="comment"># replica-announce-port 1234</span></span><br><span class="line"></span><br><span class="line"><span class="comment">############################### KEYS TRACKING #################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Redis implements server assisted support for client side caching of values.</span></span><br><span class="line"><span class="comment"># This is implemented using an invalidation table that remembers, using</span></span><br><span class="line"><span class="comment"># 16 millions of slots, what clients may have certain subsets of keys. In turn</span></span><br><span class="line"><span class="comment"># this is used in order to send invalidation messages to clients. Please</span></span><br><span class="line"><span class="comment"># to understand more about the feature check this page:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   https://redis.io/topics/client-side-caching</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># When tracking is enabled for a client, all the read only queries are assumed</span></span><br><span class="line"><span class="comment"># to be cached: this will force Redis to store information in the invalidation</span></span><br><span class="line"><span class="comment"># table. When keys are modified, such information is flushed away, and</span></span><br><span class="line"><span class="comment"># invalidation messages are sent to the clients. However if the workload is</span></span><br><span class="line"><span class="comment"># heavily dominated by reads, Redis could use more and more memory in order</span></span><br><span class="line"><span class="comment"># to track the keys fetched by many clients.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># For this reason it is possible to configure a maximum fill value for the</span></span><br><span class="line"><span class="comment"># invalidation table. By default it is set to 1M of keys, and once this limit</span></span><br><span class="line"><span class="comment"># is reached, Redis will start to evict keys in the invalidation table</span></span><br><span class="line"><span class="comment"># even if they were not modified, just to reclaim memory: this will in turn</span></span><br><span class="line"><span class="comment"># force the clients to invalidate the cached values. Basically the table</span></span><br><span class="line"><span class="comment"># maximum size is a trade off between the memory you want to spend server</span></span><br><span class="line"><span class="comment"># side to track information about who cached what, and the ability of clients</span></span><br><span class="line"><span class="comment"># to retain cached objects in memory.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># If you set the value to 0, it means there are no limits, and Redis will</span></span><br><span class="line"><span class="comment"># retain as many keys as needed in the invalidation table.</span></span><br><span class="line"><span class="comment"># In the "stats" INFO section, you can find information about the number of</span></span><br><span class="line"><span class="comment"># keys in the invalidation table at every given moment.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Note: when key tracking is used in broadcasting mode, no memory is used</span></span><br><span class="line"><span class="comment"># in the server side so this setting is useless.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># tracking-table-max-keys 1000000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################## SECURITY ###################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Warning: since Redis is pretty fast an outside user can try up to</span></span><br><span class="line"><span class="comment"># 1 million passwords per second against a modern box. This means that you</span></span><br><span class="line"><span class="comment"># should use very strong passwords, otherwise they will be very easy to break.</span></span><br><span class="line"><span class="comment"># Note that because the password is really a shared secret between the client</span></span><br><span class="line"><span class="comment"># and the server, and should not be memorized by any human, the password</span></span><br><span class="line"><span class="comment"># can be easily a long string from /dev/urandom or whatever, so by using a</span></span><br><span class="line"><span class="comment"># long and unguessable password no brute force attack will be possible.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Redis ACL users are defined in the following format:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   user &lt;username&gt; ... acl rules ...</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># For example:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   user worker +@list +@connection ~jobs:* on &gt;ffa9203c493aa99</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The special username "default" is used for new connections. If this user</span></span><br><span class="line"><span class="comment"># has the "nopass" rule, then new connections will be immediately authenticated</span></span><br><span class="line"><span class="comment"># as the "default" user without the need of any password provided via the</span></span><br><span class="line"><span class="comment"># AUTH command. Otherwise if the "default" user is not flagged with "nopass"</span></span><br><span class="line"><span class="comment"># the connections will start in not authenticated state, and will require</span></span><br><span class="line"><span class="comment"># AUTH (or the HELLO command AUTH option) in order to be authenticated and</span></span><br><span class="line"><span class="comment"># start to work.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The ACL rules that describe what an user can do are the following:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  on           Enable the user: it is possible to authenticate as this user.</span></span><br><span class="line"><span class="comment">#  off          Disable the user: it's no longer possible to authenticate</span></span><br><span class="line"><span class="comment">#               with this user, however the already authenticated connections</span></span><br><span class="line"><span class="comment">#               will still work.</span></span><br><span class="line"><span class="comment">#  +&lt;command&gt;   Allow the execution of that command</span></span><br><span class="line"><span class="comment">#  -&lt;command&gt;   Disallow the execution of that command</span></span><br><span class="line"><span class="comment">#  +@&lt;category&gt; Allow the execution of all the commands in such category</span></span><br><span class="line"><span class="comment">#               with valid categories are like @admin, @set, @sortedset, ...</span></span><br><span class="line"><span class="comment">#               and so forth, see the full list in the server.c file where</span></span><br><span class="line"><span class="comment">#               the Redis command table is described and defined.</span></span><br><span class="line"><span class="comment">#               The special category @all means all the commands, but currently</span></span><br><span class="line"><span class="comment">#               present in the server, and that will be loaded in the future</span></span><br><span class="line"><span class="comment">#               via modules.</span></span><br><span class="line"><span class="comment">#  +&lt;command&gt;|subcommand    Allow a specific subcommand of an otherwise</span></span><br><span class="line"><span class="comment">#                           disabled command. Note that this form is not</span></span><br><span class="line"><span class="comment">#                           allowed as negative like -DEBUG|SEGFAULT, but</span></span><br><span class="line"><span class="comment">#                           only additive starting with "+".</span></span><br><span class="line"><span class="comment">#  allcommands  Alias for +@all. Note that it implies the ability to execute</span></span><br><span class="line"><span class="comment">#               all the future commands loaded via the modules system.</span></span><br><span class="line"><span class="comment">#  nocommands   Alias for -@all.</span></span><br><span class="line"><span class="comment">#  ~&lt;pattern&gt;   Add a pattern of keys that can be mentioned as part of</span></span><br><span class="line"><span class="comment">#               commands. For instance ~* allows all the keys. The pattern</span></span><br><span class="line"><span class="comment">#               is a glob-style pattern like the one of KEYS.</span></span><br><span class="line"><span class="comment">#               It is possible to specify multiple patterns.</span></span><br><span class="line"><span class="comment">#  allkeys      Alias for ~*</span></span><br><span class="line"><span class="comment">#  resetkeys    Flush the list of allowed keys patterns.</span></span><br><span class="line"><span class="comment">#  &gt;&lt;password&gt;  Add this passowrd to the list of valid password for the user.</span></span><br><span class="line"><span class="comment">#               For example &gt;mypass will add "mypass" to the list.</span></span><br><span class="line"><span class="comment">#               This directive clears the "nopass" flag (see later).</span></span><br><span class="line"><span class="comment">#  &lt;&lt;password&gt;  Remove this password from the list of valid passwords.</span></span><br><span class="line"><span class="comment">#  nopass       All the set passwords of the user are removed, and the user</span></span><br><span class="line"><span class="comment">#               is flagged as requiring no password: it means that every</span></span><br><span class="line"><span class="comment">#               password will work against this user. If this directive is</span></span><br><span class="line"><span class="comment">#               used for the default user, every new connection will be</span></span><br><span class="line"><span class="comment">#               immediately authenticated with the default user without</span></span><br><span class="line"><span class="comment">#               any explicit AUTH command required. Note that the "resetpass"</span></span><br><span class="line"><span class="comment">#               directive will clear this condition.</span></span><br><span class="line"><span class="comment">#  resetpass    Flush the list of allowed passwords. Moreover removes the</span></span><br><span class="line"><span class="comment">#               "nopass" status. After "resetpass" the user has no associated</span></span><br><span class="line"><span class="comment">#               passwords and there is no way to authenticate without adding</span></span><br><span class="line"><span class="comment">#               some password (or setting it as "nopass" later).</span></span><br><span class="line"><span class="comment">#  reset        Performs the following actions: resetpass, resetkeys, off,</span></span><br><span class="line"><span class="comment">#               -@all. The user returns to the same state it has immediately</span></span><br><span class="line"><span class="comment">#               after its creation.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># ACL rules can be specified in any order: for instance you can start with</span></span><br><span class="line"><span class="comment"># passwords, then flags, or key patterns. However note that the additive</span></span><br><span class="line"><span class="comment"># and subtractive rules will CHANGE MEANING depending on the ordering.</span></span><br><span class="line"><span class="comment"># For instance see the following example:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   user alice on +@all -DEBUG ~* &gt;somepassword</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># This will allow "alice" to use all the commands with the exception of the</span></span><br><span class="line"><span class="comment"># DEBUG command, since +@all added all the commands to the set of the commands</span></span><br><span class="line"><span class="comment"># alice can use, and later DEBUG was removed. However if we invert the order</span></span><br><span class="line"><span class="comment"># of two ACL rules the result will be different:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   user alice on -DEBUG +@all ~* &gt;somepassword</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Now DEBUG was removed when alice had yet no commands in the set of allowed</span></span><br><span class="line"><span class="comment"># commands, later all the commands are added, so the user will be able to</span></span><br><span class="line"><span class="comment"># execute everything.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Basically ACL rules are processed left-to-right.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># For more information about ACL configuration please refer to</span></span><br><span class="line"><span class="comment"># the Redis web site at https://redis.io/topics/acl</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ACL LOG</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The ACL Log tracks failed commands and authentication events associated</span></span><br><span class="line"><span class="comment"># with ACLs. The ACL Log is useful to troubleshoot failed commands blocked </span></span><br><span class="line"><span class="comment"># by ACLs. The ACL Log is stored in and consumes memory. There is no limit</span></span><br><span class="line"><span class="comment"># to its length.You can reclaim memory with ACL LOG RESET or set a maximum</span></span><br><span class="line"><span class="comment"># length below.</span></span><br><span class="line"><span class="string">acllog-max-len</span> <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Using an external ACL file</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Instead of configuring users here in this file, it is possible to use</span></span><br><span class="line"><span class="comment"># a stand-alone file just listing users. The two methods cannot be mixed:</span></span><br><span class="line"><span class="comment"># if you configure users here and at the same time you activate the exteranl</span></span><br><span class="line"><span class="comment"># ACL file, the server will refuse to start.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The format of the external ACL user file is exactly the same as the</span></span><br><span class="line"><span class="comment"># format that is used inside redis.conf to describe users.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># aclfile /etc/redis/users.acl</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># IMPORTANT <span class="doctag">NOTE:</span> starting with Redis 6 "requirepass" is just a compatiblity</span></span><br><span class="line"><span class="comment"># layer on top of the new ACL system. The option effect will be just setting</span></span><br><span class="line"><span class="comment"># the password for the default user. Clients will still authenticate using</span></span><br><span class="line"><span class="comment"># AUTH &lt;password&gt; as usually, or more explicitly with AUTH default &lt;password&gt;</span></span><br><span class="line"><span class="comment"># if they follow the new protocol: both will work.</span></span><br><span class="line"><span class="comment"># 设置Redis连接密码，如果配置了连接密码，客户端在连接Redis时需要通过auth &lt;password&gt;命令提供密码，默认关闭</span></span><br><span class="line"><span class="comment"># requirepass 123456</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Command renaming (DEPRECATED).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># ------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># WARNING: avoid using this option if possible. Instead use ACLs to remove</span></span><br><span class="line"><span class="comment"># commands from the default user, and put them only in some admin user you</span></span><br><span class="line"><span class="comment"># create for administrative purposes.</span></span><br><span class="line"><span class="comment"># ------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># It is possible to change the name of dangerous commands in a shared</span></span><br><span class="line"><span class="comment"># environment. For instance the CONFIG command may be renamed into something</span></span><br><span class="line"><span class="comment"># hard to guess so that it will still be available for internal-use tools</span></span><br><span class="line"><span class="comment"># but not available for general clients.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Example:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># It is also possible to completely kill a command by renaming it into</span></span><br><span class="line"><span class="comment"># an empty string:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># rename-command CONFIG ""</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Please note that changing the name of commands that are logged into the</span></span><br><span class="line"><span class="comment"># AOF file or transmitted to replicas may cause problems.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################### CLIENTS ####################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the max number of connected clients at the same time. By default</span></span><br><span class="line"><span class="comment"># this limit is set to 10000 clients, however if the Redis server is not</span></span><br><span class="line"><span class="comment"># able to configure the process file limit to allow for the specified limit</span></span><br><span class="line"><span class="comment"># the max number of allowed clients is set to the current file limit</span></span><br><span class="line"><span class="comment"># minus 32 (as Redis reserves a few file descriptors for internal uses).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Once the limit is reached Redis will close all the new connections sending</span></span><br><span class="line"><span class="comment"># an error 'max number of clients reached'.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># maxclients 10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">############################## MEMORY MANAGEMENT ################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set a memory usage limit to the specified amount of bytes.</span></span><br><span class="line"><span class="comment"># When the memory limit is reached Redis will try to remove keys</span></span><br><span class="line"><span class="comment"># according to the eviction policy selected (see maxmemory-policy).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># If Redis can't remove keys according to the policy, or if the policy is</span></span><br><span class="line"><span class="comment"># set to 'noeviction', Redis will start to reply with errors to commands</span></span><br><span class="line"><span class="comment"># that would use more memory, like SET, LPUSH, and so on, and will continue</span></span><br><span class="line"><span class="comment"># to reply to read-only commands like GET.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># This option is usually useful when using Redis as an LRU or LFU cache, or to</span></span><br><span class="line"><span class="comment"># set a hard memory limit for an instance (using the 'noeviction' policy).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># WARNING: If you have replicas attached to an instance with maxmemory on,</span></span><br><span class="line"><span class="comment"># the size of the output buffers needed to feed the replicas are subtracted</span></span><br><span class="line"><span class="comment"># from the used memory count, so that network problems / resyncs will</span></span><br><span class="line"><span class="comment"># not trigger a loop where keys are evicted, and in turn the output</span></span><br><span class="line"><span class="comment"># buffer of replicas is full with DELs of keys evicted triggering the deletion</span></span><br><span class="line"><span class="comment"># of more keys, and so forth until the database is completely emptied.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># In short... if you have replicas attached it is suggested that you set a lower</span></span><br><span class="line"><span class="comment"># limit for maxmemory so that there is some free RAM on the system for replica</span></span><br><span class="line"><span class="comment"># output buffers (but this is not needed if the policy is 'noeviction').</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#设置redis占用最大内存数，如果超过redis会试图删除即将过期的key，而保护具有较长生命周期的key</span></span><br><span class="line"><span class="string">maxmemory</span> <span class="number">5</span><span class="string">gb</span></span><br><span class="line"><span class="comment"># maxmemory &lt;bytes&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MAXMEMORY POLICY: how Redis will select what to remove when maxmemory</span></span><br><span class="line"><span class="comment"># is reached. You can select one from the following behaviors:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># volatile-lru -&gt; Evict using approximated LRU, only keys with an expire set.</span></span><br><span class="line"><span class="comment"># allkeys-lru -&gt; Evict any key using approximated LRU.</span></span><br><span class="line"><span class="comment"># volatile-lfu -&gt; Evict using approximated LFU, only keys with an expire set.</span></span><br><span class="line"><span class="comment"># allkeys-lfu -&gt; Evict any key using approximated LFU.</span></span><br><span class="line"><span class="comment"># volatile-random -&gt; Remove a random key having an expire set.</span></span><br><span class="line"><span class="comment"># allkeys-random -&gt; Remove a random key, any key.</span></span><br><span class="line"><span class="comment"># volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL)</span></span><br><span class="line"><span class="comment"># noeviction -&gt; Don't evict anything, just return an error on write operations.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># LRU means Least Recently Used</span></span><br><span class="line"><span class="comment"># LFU means Least Frequently Used</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Both LRU, LFU and volatile-ttl are implemented using approximated</span></span><br><span class="line"><span class="comment"># randomized algorithms.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Note: with any of the above policies, Redis will return an error on write</span></span><br><span class="line"><span class="comment">#       operations, when there are no suitable keys for eviction.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#       At the date of writing these commands are: set setnx setex append</span></span><br><span class="line"><span class="comment">#       incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd</span></span><br><span class="line"><span class="comment">#       sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby</span></span><br><span class="line"><span class="comment">#       zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby</span></span><br><span class="line"><span class="comment">#       getset mset msetnx exec sort</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The default is:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#当内存占用超过maxmemory限定时，触发主动清理策略</span></span><br><span class="line"><span class="comment">#清理策略方式如下：</span></span><br><span class="line"><span class="comment">#volatile-lru：只对设置了过期时间的key进行LRU（默认值）</span></span><br><span class="line"><span class="comment">#allkeys-lru ： 删除lru算法的key</span></span><br><span class="line"><span class="comment">#volatile-random：随机删除即将过期key</span></span><br><span class="line"><span class="comment">#allkeys-random：随机删除</span></span><br><span class="line"><span class="comment">#volatile-ttl ： 删除即将过期的</span></span><br><span class="line"><span class="comment">#noeviction ： 永不过期，返回错误</span></span><br><span class="line"><span class="string">maxmemory-policy</span> <span class="string">allkeys-lru</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated</span></span><br><span class="line"><span class="comment"># algorithms (in order to save memory), so you can tune it for speed or</span></span><br><span class="line"><span class="comment"># accuracy. For default Redis will check five keys and pick the one that was</span></span><br><span class="line"><span class="comment"># used less recently, you can change the sample size using the following</span></span><br><span class="line"><span class="comment"># configuration directive.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The default of 5 produces good enough results. 10 Approximates very closely</span></span><br><span class="line"><span class="comment"># true LRU but costs more CPU. 3 is faster but not very accurate.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># maxmemory-samples 5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Starting from Redis 5, by default a replica will ignore its maxmemory setting</span></span><br><span class="line"><span class="comment"># (unless it is promoted to master after a failover or manually). It means</span></span><br><span class="line"><span class="comment"># that the eviction of keys will be just handled by the master, sending the</span></span><br><span class="line"><span class="comment"># DEL commands to the replica as keys evict in the master side.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># This behavior ensures that masters and replicas stay consistent, and is usually</span></span><br><span class="line"><span class="comment"># what you want, however if your replica is writable, or you want the replica</span></span><br><span class="line"><span class="comment"># to have a different memory setting, and you are sure all the writes performed</span></span><br><span class="line"><span class="comment"># to the replica are idempotent, then you may change this default (but be sure</span></span><br><span class="line"><span class="comment"># to understand what you are doing).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Note that since the replica by default does not evict, it may end using more</span></span><br><span class="line"><span class="comment"># memory than the one set via maxmemory (there are certain buffers that may</span></span><br><span class="line"><span class="comment"># be larger on the replica, or data structures may sometimes take more memory</span></span><br><span class="line"><span class="comment"># and so forth). So make sure you monitor your replicas and make sure they</span></span><br><span class="line"><span class="comment"># have enough memory to never hit a real out-of-memory condition before the</span></span><br><span class="line"><span class="comment"># master hits the configured maxmemory setting.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># replica-ignore-maxmemory yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Redis reclaims expired keys in two ways: upon access when those keys are</span></span><br><span class="line"><span class="comment"># found to be expired, and also in background, in what is called the</span></span><br><span class="line"><span class="comment"># "active expire key". The key space is slowly and interactively scanned</span></span><br><span class="line"><span class="comment"># looking for expired keys to reclaim, so that it is possible to free memory</span></span><br><span class="line"><span class="comment"># of keys that are expired and will never be accessed again in a short time.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The default effort of the expire cycle will try to avoid having more than</span></span><br><span class="line"><span class="comment"># ten percent of expired keys still in memory, and will try to avoid consuming</span></span><br><span class="line"><span class="comment"># more than 25% of total memory and to add latency to the system. However</span></span><br><span class="line"><span class="comment"># it is possible to increase the expire "effort" that is normally set to</span></span><br><span class="line"><span class="comment"># "1", to a greater value, up to the value "10". At its maximum value the</span></span><br><span class="line"><span class="comment"># system will use more CPU, longer cycles (and technically may introduce</span></span><br><span class="line"><span class="comment"># more latency), and will tollerate less already expired keys still present</span></span><br><span class="line"><span class="comment"># in the system. It's a tradeoff betweeen memory, CPU and latecy.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># active-expire-effort 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">############################# LAZY FREEING ####################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Redis has two primitives to delete keys. One is called DEL and is a blocking</span></span><br><span class="line"><span class="comment"># deletion of the object. It means that the server stops processing new commands</span></span><br><span class="line"><span class="comment"># in order to reclaim all the memory associated with an object in a synchronous</span></span><br><span class="line"><span class="comment"># way. If the key deleted is associated with a small object, the time needed</span></span><br><span class="line"><span class="comment"># in order to execute the DEL command is very small and comparable to most other</span></span><br><span class="line"><span class="comment"># O(1) or O(log_N) commands in Redis. However if the key is associated with an</span></span><br><span class="line"><span class="comment"># aggregated value containing millions of elements, the server can block for</span></span><br><span class="line"><span class="comment"># a long time (even seconds) in order to complete the operation.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># For the above reasons Redis also offers non blocking deletion primitives</span></span><br><span class="line"><span class="comment"># such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and</span></span><br><span class="line"><span class="comment"># FLUSHDB commands, in order to reclaim memory in background. Those commands</span></span><br><span class="line"><span class="comment"># are executed in constant time. Another thread will incrementally free the</span></span><br><span class="line"><span class="comment"># object in the background as fast as possible.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.</span></span><br><span class="line"><span class="comment"># It's up to the design of the application to understand when it is a good</span></span><br><span class="line"><span class="comment"># idea to use one or the other. However the Redis server sometimes has to</span></span><br><span class="line"><span class="comment"># delete keys or flush the whole database as a side effect of other operations.</span></span><br><span class="line"><span class="comment"># Specifically Redis deletes objects independently of a user call in the</span></span><br><span class="line"><span class="comment"># following scenarios:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 1) On eviction, because of the maxmemory and maxmemory policy configurations,</span></span><br><span class="line"><span class="comment">#    in order to make room for new data, without going over the specified</span></span><br><span class="line"><span class="comment">#    memory limit.</span></span><br><span class="line"><span class="comment"># 2) Because of expire: when a key with an associated time to live (see the</span></span><br><span class="line"><span class="comment">#    EXPIRE command) must be deleted from memory.</span></span><br><span class="line"><span class="comment"># 3) Because of a side effect of a command that stores data on a key that may</span></span><br><span class="line"><span class="comment">#    already exist. For example the RENAME command may delete the old key</span></span><br><span class="line"><span class="comment">#    content when it is replaced with another one. Similarly SUNIONSTORE</span></span><br><span class="line"><span class="comment">#    or SORT with STORE option may delete existing keys. The SET command</span></span><br><span class="line"><span class="comment">#    itself removes any old content of the specified key in order to replace</span></span><br><span class="line"><span class="comment">#    it with the specified string.</span></span><br><span class="line"><span class="comment"># 4) During replication, when a replica performs a full resynchronization with</span></span><br><span class="line"><span class="comment">#    its master, the content of the whole database is removed in order to</span></span><br><span class="line"><span class="comment">#    load the RDB file just transferred.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># In all the above cases the default is to delete objects in a blocking way,</span></span><br><span class="line"><span class="comment"># like if DEL was called. However you can configure each case specifically</span></span><br><span class="line"><span class="comment"># in order to instead release memory in a non-blocking way like if UNLINK</span></span><br><span class="line"><span class="comment"># was called, using the following configuration directives.</span></span><br><span class="line"></span><br><span class="line"><span class="string">lazyfree-lazy-eviction</span> <span class="literal">no</span></span><br><span class="line"><span class="string">lazyfree-lazy-expire</span> <span class="literal">no</span></span><br><span class="line"><span class="string">lazyfree-lazy-server-del</span> <span class="literal">no</span></span><br><span class="line"><span class="string">replica-lazy-flush</span> <span class="literal">no</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># It is also possible, for the case when to replace the user code DEL calls</span></span><br><span class="line"><span class="comment"># with UNLINK calls is not easy, to modify the default behavior of the DEL</span></span><br><span class="line"><span class="comment"># command to act exactly like UNLINK, using the following configuration</span></span><br><span class="line"><span class="comment"># directive:</span></span><br><span class="line"></span><br><span class="line"><span class="string">lazyfree-lazy-user-del</span> <span class="literal">no</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################ THREADED I/O #################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Redis is mostly single threaded, however there are certain threaded</span></span><br><span class="line"><span class="comment"># operations such as UNLINK, slow I/O accesses and other things that are</span></span><br><span class="line"><span class="comment"># performed on side threads.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Now it is also possible to handle Redis clients socket reads and writes</span></span><br><span class="line"><span class="comment"># in different I/O threads. Since especially writing is so slow, normally</span></span><br><span class="line"><span class="comment"># Redis users use pipelining in order to speedup the Redis performances per</span></span><br><span class="line"><span class="comment"># core, and spawn multiple instances in order to scale more. Using I/O</span></span><br><span class="line"><span class="comment"># threads it is possible to easily speedup two times Redis without resorting</span></span><br><span class="line"><span class="comment"># to pipelining nor sharding of the instance.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># By default threading is disabled, we suggest enabling it only in machines</span></span><br><span class="line"><span class="comment"># that have at least 4 or more cores, leaving at least one spare core.</span></span><br><span class="line"><span class="comment"># Using more than 8 threads is unlikely to help much. We also recommend using</span></span><br><span class="line"><span class="comment"># threaded I/O only if you actually have performance problems, with Redis</span></span><br><span class="line"><span class="comment"># instances being able to use a quite big percentage of CPU time, otherwise</span></span><br><span class="line"><span class="comment"># there is no point in using this feature.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># So for instance if you have a four cores boxes, try to use 2 or 3 I/O</span></span><br><span class="line"><span class="comment"># threads, if you have a 8 cores, try to use 6 threads. In order to</span></span><br><span class="line"><span class="comment"># enable I/O threads use the following configuration directive:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># io-threads 4</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Setting io-threads to 1 will just use the main thread as usually.</span></span><br><span class="line"><span class="comment"># When I/O threads are enabled, we only use threads for writes, that is</span></span><br><span class="line"><span class="comment"># to thread the write(2) syscall and transfer the client buffers to the</span></span><br><span class="line"><span class="comment"># socket. However it is also possible to enable threading of reads and</span></span><br><span class="line"><span class="comment"># protocol parsing using the following configuration directive, by setting</span></span><br><span class="line"><span class="comment"># it to yes:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># io-threads-do-reads no</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Usually threading reads doesn't help much.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># NOTE 1: This configuration directive cannot be changed at runtime via</span></span><br><span class="line"><span class="comment"># CONFIG SET. Aso this feature currently does not work when SSL is</span></span><br><span class="line"><span class="comment"># enabled.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># NOTE 2: If you want to test the Redis speedup using redis-benchmark, make</span></span><br><span class="line"><span class="comment"># sure you also run the benchmark itself in threaded mode, using the</span></span><br><span class="line"><span class="comment"># --threads option to match the number of Redis theads, otherwise you'll not</span></span><br><span class="line"><span class="comment"># be able to notice the improvements.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">############################## APPEND ONLY MODE ###############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># By default Redis asynchronously dumps the dataset on disk. This mode is</span></span><br><span class="line"><span class="comment"># good enough in many applications, but an issue with the Redis process or</span></span><br><span class="line"><span class="comment"># a power outage may result into a few minutes of writes lost (depending on</span></span><br><span class="line"><span class="comment"># the configured save points).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The Append Only File is an alternative persistence mode that provides</span></span><br><span class="line"><span class="comment"># much better durability. For instance using the default data fsync policy</span></span><br><span class="line"><span class="comment"># (see later in the config file) Redis can lose just one second of writes in a</span></span><br><span class="line"><span class="comment"># dramatic event like a server power outage, or a single write if something</span></span><br><span class="line"><span class="comment"># wrong with the Redis process itself happens, but the operating system is</span></span><br><span class="line"><span class="comment"># still running correctly.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># AOF and RDB persistence can be enabled at the same time without problems.</span></span><br><span class="line"><span class="comment"># If the AOF is enabled on startup Redis will load the AOF, that is the file</span></span><br><span class="line"><span class="comment"># with the better durability guarantees.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Please check http://redis.io/topics/persistence for more information.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#是否开启aof功能,"yes"表示开启,在开启情况下,aof文件同步功能才生效,默认为"no",对master机器,建议使用AOF,对于slave,建议关闭</span></span><br><span class="line"><span class="string">appendonly</span> <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The name of the append only file (default: "appendonly.aof")</span></span><br><span class="line"></span><br><span class="line"><span class="string">appendfilename</span> <span class="string">"appendonly.aof"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The fsync() call tells the Operating System to actually write data on disk</span></span><br><span class="line"><span class="comment"># instead of waiting for more data in the output buffer. Some OS will really flush</span></span><br><span class="line"><span class="comment"># data on disk, some other OS will just try to do it ASAP.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Redis supports three different modes:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># no: don't fsync, just let the OS flush the data when it wants. Faster.</span></span><br><span class="line"><span class="comment"># always: fsync after every write to the append only log. Slow, Safest.</span></span><br><span class="line"><span class="comment"># everysec: fsync only one time every second. Compromise.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The default is "everysec", as that's usually the right compromise between</span></span><br><span class="line"><span class="comment"># speed and data safety. It's up to you to understand if you can relax this to</span></span><br><span class="line"><span class="comment"># "no" that will let the operating system flush the output buffer when</span></span><br><span class="line"><span class="comment"># it wants, for better performances (but if you can live with the idea of</span></span><br><span class="line"><span class="comment"># some data loss consider the default persistence mode that's snapshotting),</span></span><br><span class="line"><span class="comment"># or on the contrary, use "always" that's very slow but a bit safer than</span></span><br><span class="line"><span class="comment"># everysec.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># More details please check the following article:</span></span><br><span class="line"><span class="comment"># http://antirez.com/post/redis-persistence-demystified.html</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># If unsure, use "everysec".</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># appendfsync always</span></span><br><span class="line"><span class="comment">#任何一个aof记录都立即进行文件同步(磁盘写入),安全性最高;如果write请求比较密集,将会造成较高的磁盘IO开支和响应延迟，everysec每秒同步一次</span></span><br><span class="line"><span class="string">appendfsync</span> <span class="string">everysec</span></span><br><span class="line"><span class="comment"># appendfsync no</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># When the AOF fsync policy is set to always or everysec, and a background</span></span><br><span class="line"><span class="comment"># saving process (a background save or AOF log background rewriting) is</span></span><br><span class="line"><span class="comment"># performing a lot of I/O against the disk, in some Linux configurations</span></span><br><span class="line"><span class="comment"># Redis may block too long on the fsync() call. Note that there is no fix for</span></span><br><span class="line"><span class="comment"># this currently, as even performing fsync in a different thread will block</span></span><br><span class="line"><span class="comment"># our synchronous write(2) call.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># In order to mitigate this problem it's possible to use the following option</span></span><br><span class="line"><span class="comment"># that will prevent fsync() from being called in the main process while a</span></span><br><span class="line"><span class="comment"># BGSAVE or BGREWRITEAOF is in progress.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># This means that while another child is saving, the durability of Redis is</span></span><br><span class="line"><span class="comment"># the same as "appendfsync none". In practical terms, this means that it is</span></span><br><span class="line"><span class="comment"># possible to lose up to 30 seconds of log in the worst scenario (with the</span></span><br><span class="line"><span class="comment"># default Linux settings).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># If you have latency problems turn this to "yes". Otherwise leave it as</span></span><br><span class="line"><span class="comment"># "no" that is the safest pick from the point of view of durability.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#在aof rewrite期间,是否对aof新记录的append暂缓使用文件同步策略,主要考虑磁盘IO开支和请求阻塞时间，默认为no,表示"不暂缓",新的aof记录仍然会被立即同步</span></span><br><span class="line"><span class="literal">no</span><span class="bullet">-appendfsync-on-rewrite</span> <span class="literal">no</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Automatic rewrite of the append only file.</span></span><br><span class="line"><span class="comment"># Redis is able to automatically rewrite the log file implicitly calling</span></span><br><span class="line"><span class="comment"># BGREWRITEAOF when the AOF log size grows by the specified percentage.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># This is how it works: Redis remembers the size of the AOF file after the</span></span><br><span class="line"><span class="comment"># latest rewrite (if no rewrite has happened since the restart, the size of</span></span><br><span class="line"><span class="comment"># the AOF at startup is used).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># This base size is compared to the current size. If the current size is</span></span><br><span class="line"><span class="comment"># bigger than the specified percentage, the rewrite is triggered. Also</span></span><br><span class="line"><span class="comment"># you need to specify a minimal size for the AOF file to be rewritten, this</span></span><br><span class="line"><span class="comment"># is useful to avoid rewriting the AOF file even if the percentage increase</span></span><br><span class="line"><span class="comment"># is reached but it is still pretty small.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Specify a percentage of zero in order to disable the automatic AOF</span></span><br><span class="line"><span class="comment"># rewrite feature.</span></span><br><span class="line"><span class="comment">#aof每次rewrite之后，都会记住当前aof文件的大小，当文件增长到一定比例后，继续进行aof rewrite</span></span><br><span class="line"><span class="string">auto-aof-rewrite-percentage</span> <span class="number">100</span></span><br><span class="line"><span class="comment">#aof rewrite触发时机，最小文件尺寸</span></span><br><span class="line"><span class="string">auto-aof-rewrite-min-size</span> <span class="number">64</span><span class="string">mb</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># An AOF file may be found to be truncated at the end during the Redis</span></span><br><span class="line"><span class="comment"># startup process, when the AOF data gets loaded back into memory.</span></span><br><span class="line"><span class="comment"># This may happen when the system where Redis is running</span></span><br><span class="line"><span class="comment"># crashes, especially when an ext4 filesystem is mounted without the</span></span><br><span class="line"><span class="comment"># data=ordered option (however this can't happen when Redis itself</span></span><br><span class="line"><span class="comment"># crashes or aborts but the operating system still works correctly).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Redis can either exit with an error when this happens, or load as much</span></span><br><span class="line"><span class="comment"># data as possible (the default now) and start if the AOF file is found</span></span><br><span class="line"><span class="comment"># to be truncated at the end. The following option controls this behavior.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># If aof-load-truncated is set to yes, a truncated AOF file is loaded and</span></span><br><span class="line"><span class="comment"># the Redis server starts emitting a log to inform the user of the event.</span></span><br><span class="line"><span class="comment"># Otherwise if the option is set to no, the server aborts with an error</span></span><br><span class="line"><span class="comment"># and refuses to start. When the option is set to no, the user requires</span></span><br><span class="line"><span class="comment"># to fix the AOF file using the "redis-check-aof" utility before to restart</span></span><br><span class="line"><span class="comment"># the server.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Note that if the AOF file will be found to be corrupted in the middle</span></span><br><span class="line"><span class="comment"># the server will still exit with an error. This option only applies when</span></span><br><span class="line"><span class="comment"># Redis will try to read more data from the AOF file but not enough bytes</span></span><br><span class="line"><span class="comment"># will be found.</span></span><br><span class="line"><span class="string">aof-load-truncated</span> <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># When rewriting the AOF file, Redis is able to use an RDB preamble in the</span></span><br><span class="line"><span class="comment"># AOF file for faster rewrites and recoveries. When this option is turned</span></span><br><span class="line"><span class="comment"># on the rewritten AOF file is composed of two different stanzas:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   [RDB file][AOF tail]</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># When loading Redis recognizes that the AOF file starts with the "REDIS"</span></span><br><span class="line"><span class="comment"># string and loads the prefixed RDB file, and continues loading the AOF</span></span><br><span class="line"><span class="comment"># tail.</span></span><br><span class="line"><span class="string">aof-use-rdb-preamble</span> <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################ LUA SCRIPTING  ###############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Max execution time of a Lua script in milliseconds.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># If the maximum execution time is reached Redis will log that a script is</span></span><br><span class="line"><span class="comment"># still in execution after the maximum allowed time and will start to</span></span><br><span class="line"><span class="comment"># reply to queries with an error.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># When a long running script exceeds the maximum execution time only the</span></span><br><span class="line"><span class="comment"># SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be</span></span><br><span class="line"><span class="comment"># used to stop a script that did not yet called write commands. The second</span></span><br><span class="line"><span class="comment"># is the only way to shut down the server in the case a write command was</span></span><br><span class="line"><span class="comment"># already issued by the script but the user doesn't want to wait for the natural</span></span><br><span class="line"><span class="comment"># termination of the script.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Set it to 0 or a negative value for unlimited execution without warnings.</span></span><br><span class="line"><span class="comment">#lua脚本运行的最大时间</span></span><br><span class="line"><span class="string">lua-time-limit</span> <span class="number">5000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################ REDIS CLUSTER  ###############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Normal Redis instances can't be part of a Redis Cluster; only nodes that are</span></span><br><span class="line"><span class="comment"># started as cluster nodes can. In order to start a Redis instance as a</span></span><br><span class="line"><span class="comment"># cluster node enable the cluster support uncommenting the following:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># cluster-enabled yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Every cluster node has a cluster configuration file. This file is not</span></span><br><span class="line"><span class="comment"># intended to be edited by hand. It is created and updated by Redis nodes.</span></span><br><span class="line"><span class="comment"># Every Redis Cluster node requires a different cluster configuration file.</span></span><br><span class="line"><span class="comment"># Make sure that instances running in the same system do not have</span></span><br><span class="line"><span class="comment"># overlapping cluster configuration file names.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># cluster-config-file nodes-6379.conf</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Cluster node timeout is the amount of milliseconds a node must be unreachable</span></span><br><span class="line"><span class="comment"># for it to be considered in failure state.</span></span><br><span class="line"><span class="comment"># Most other internal time limits are multiple of the node timeout.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># cluster-node-timeout 15000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A replica of a failing master will avoid to start a failover if its data</span></span><br><span class="line"><span class="comment"># looks too old.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># There is no simple way for a replica to actually have an exact measure of</span></span><br><span class="line"><span class="comment"># its "data age", so the following two checks are performed:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 1) If there are multiple replicas able to failover, they exchange messages</span></span><br><span class="line"><span class="comment">#    in order to try to give an advantage to the replica with the best</span></span><br><span class="line"><span class="comment">#    replication offset (more data from the master processed).</span></span><br><span class="line"><span class="comment">#    Replicas will try to get their rank by offset, and apply to the start</span></span><br><span class="line"><span class="comment">#    of the failover a delay proportional to their rank.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 2) Every single replica computes the time of the last interaction with</span></span><br><span class="line"><span class="comment">#    its master. This can be the last ping or command received (if the master</span></span><br><span class="line"><span class="comment">#    is still in the "connected" state), or the time that elapsed since the</span></span><br><span class="line"><span class="comment">#    disconnection with the master (if the replication link is currently down).</span></span><br><span class="line"><span class="comment">#    If the last interaction is too old, the replica will not try to failover</span></span><br><span class="line"><span class="comment">#    at all.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The point "2" can be tuned by user. Specifically a replica will not perform</span></span><br><span class="line"><span class="comment"># the failover if, since the last interaction with the master, the time</span></span><br><span class="line"><span class="comment"># elapsed is greater than:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   (node-timeout * replica-validity-factor) + repl-ping-replica-period</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># So for example if node-timeout is 30 seconds, and the replica-validity-factor</span></span><br><span class="line"><span class="comment"># is 10, and assuming a default repl-ping-replica-period of 10 seconds, the</span></span><br><span class="line"><span class="comment"># replica will not try to failover if it was not able to talk with the master</span></span><br><span class="line"><span class="comment"># for longer than 310 seconds.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># A large replica-validity-factor may allow replicas with too old data to failover</span></span><br><span class="line"><span class="comment"># a master, while a too small value may prevent the cluster from being able to</span></span><br><span class="line"><span class="comment"># elect a replica at all.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># For maximum availability, it is possible to set the replica-validity-factor</span></span><br><span class="line"><span class="comment"># to a value of 0, which means, that replicas will always try to failover the</span></span><br><span class="line"><span class="comment"># master regardless of the last time they interacted with the master.</span></span><br><span class="line"><span class="comment"># (However they'll always try to apply a delay proportional to their</span></span><br><span class="line"><span class="comment"># offset rank).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Zero is the only value able to guarantee that when all the partitions heal</span></span><br><span class="line"><span class="comment"># the cluster will always be able to continue.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># cluster-replica-validity-factor 10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Cluster replicas are able to migrate to orphaned masters, that are masters</span></span><br><span class="line"><span class="comment"># that are left without working replicas. This improves the cluster ability</span></span><br><span class="line"><span class="comment"># to resist to failures as otherwise an orphaned master can't be failed over</span></span><br><span class="line"><span class="comment"># in case of failure if it has no working replicas.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Replicas migrate to orphaned masters only if there are still at least a</span></span><br><span class="line"><span class="comment"># given number of other working replicas for their old master. This number</span></span><br><span class="line"><span class="comment"># is the "migration barrier". A migration barrier of 1 means that a replica</span></span><br><span class="line"><span class="comment"># will migrate only if there is at least 1 other working replica for its master</span></span><br><span class="line"><span class="comment"># and so forth. It usually reflects the number of replicas you want for every</span></span><br><span class="line"><span class="comment"># master in your cluster.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Default is 1 (replicas migrate only if their masters remain with at least</span></span><br><span class="line"><span class="comment"># one replica). To disable migration just set it to a very large value.</span></span><br><span class="line"><span class="comment"># A value of 0 can be set but is useful only for debugging and dangerous</span></span><br><span class="line"><span class="comment"># in production.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># cluster-migration-barrier 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># By default Redis Cluster nodes stop accepting queries if they detect there</span></span><br><span class="line"><span class="comment"># is at least an hash slot uncovered (no available node is serving it).</span></span><br><span class="line"><span class="comment"># This way if the cluster is partially down (for example a range of hash slots</span></span><br><span class="line"><span class="comment"># are no longer covered) all the cluster becomes, eventually, unavailable.</span></span><br><span class="line"><span class="comment"># It automatically returns available as soon as all the slots are covered again.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># However sometimes you want the subset of the cluster which is working,</span></span><br><span class="line"><span class="comment"># to continue to accept queries for the part of the key space that is still</span></span><br><span class="line"><span class="comment"># covered. In order to do so, just set the cluster-require-full-coverage</span></span><br><span class="line"><span class="comment"># option to no.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># cluster-require-full-coverage yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This option, when set to yes, prevents replicas from trying to failover its</span></span><br><span class="line"><span class="comment"># master during master failures. However the master can still perform a</span></span><br><span class="line"><span class="comment"># manual failover, if forced to do so.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># This is useful in different scenarios, especially in the case of multiple</span></span><br><span class="line"><span class="comment"># data center operations, where we want one side to never be promoted if not</span></span><br><span class="line"><span class="comment"># in the case of a total DC failure.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># cluster-replica-no-failover no</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This option, when set to yes, allows nodes to serve read traffic while the</span></span><br><span class="line"><span class="comment"># the cluster is in a down state, as long as it believes it owns the slots. </span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># This is useful for two cases.  The first case is for when an application </span></span><br><span class="line"><span class="comment"># doesn't require consistency of data during node failures or network partitions.</span></span><br><span class="line"><span class="comment"># One example of this is a cache, where as long as the node has the data it</span></span><br><span class="line"><span class="comment"># should be able to serve it. </span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The second use case is for configurations that don't meet the recommended  </span></span><br><span class="line"><span class="comment"># three shards but want to enable cluster mode and scale later. A </span></span><br><span class="line"><span class="comment"># master outage in a 1 or 2 shard configuration causes a read/write outage to the</span></span><br><span class="line"><span class="comment"># entire cluster without this option set, with it set there is only a write outage.</span></span><br><span class="line"><span class="comment"># Without a quorum of masters, slot ownership will not change automatically. </span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># cluster-allow-reads-when-down no</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In order to setup your cluster make sure to read the documentation</span></span><br><span class="line"><span class="comment"># available at http://redis.io web site.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">########################## CLUSTER DOCKER/NAT support  ########################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In certain deployments, Redis Cluster nodes address discovery fails, because</span></span><br><span class="line"><span class="comment"># addresses are NAT-ted or because ports are forwarded (the typical case is</span></span><br><span class="line"><span class="comment"># Docker and other containers).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># In order to make Redis Cluster working in such environments, a static</span></span><br><span class="line"><span class="comment"># configuration where each node knows its public address is needed. The</span></span><br><span class="line"><span class="comment"># following two options are used for this scope, and are:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># * cluster-announce-ip</span></span><br><span class="line"><span class="comment"># * cluster-announce-port</span></span><br><span class="line"><span class="comment"># * cluster-announce-bus-port</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Each instruct the node about its address, client port, and cluster message</span></span><br><span class="line"><span class="comment"># bus port. The information is then published in the header of the bus packets</span></span><br><span class="line"><span class="comment"># so that other nodes will be able to correctly map the address of the node</span></span><br><span class="line"><span class="comment"># publishing the information.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># If the above options are not used, the normal Redis Cluster auto-detection</span></span><br><span class="line"><span class="comment"># will be used instead.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Note that when remapped, the bus port may not be at the fixed offset of</span></span><br><span class="line"><span class="comment"># clients port + 10000, so you can specify any port and bus-port depending</span></span><br><span class="line"><span class="comment"># on how they get remapped. If the bus-port is not set, a fixed offset of</span></span><br><span class="line"><span class="comment"># 10000 will be used as usually.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Example:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># cluster-announce-ip 10.1.1.5</span></span><br><span class="line"><span class="comment"># cluster-announce-port 6379</span></span><br><span class="line"><span class="comment"># cluster-announce-bus-port 6380</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################## SLOW LOG ###################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The Redis Slow Log is a system to log queries that exceeded a specified</span></span><br><span class="line"><span class="comment"># execution time. The execution time does not include the I/O operations</span></span><br><span class="line"><span class="comment"># like talking with the client, sending the reply and so forth,</span></span><br><span class="line"><span class="comment"># but just the time needed to actually execute the command (this is the only</span></span><br><span class="line"><span class="comment"># stage of command execution where the thread is blocked and can not serve</span></span><br><span class="line"><span class="comment"># other requests in the meantime).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># You can configure the slow log with two parameters: one tells Redis</span></span><br><span class="line"><span class="comment"># what is the execution time, in microseconds, to exceed in order for the</span></span><br><span class="line"><span class="comment"># command to get logged, and the other parameter is the length of the</span></span><br><span class="line"><span class="comment"># slow log. When a new command is logged the oldest one is removed from the</span></span><br><span class="line"><span class="comment"># queue of logged commands.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The following time is expressed in microseconds, so 1000000 is equivalent</span></span><br><span class="line"><span class="comment"># to one second. Note that a negative number disables the slow log, while</span></span><br><span class="line"><span class="comment"># a value of zero forces the logging of every command.</span></span><br><span class="line"><span class="comment">#慢操作日志记录</span></span><br><span class="line"><span class="string">slowlog-log-slower-than</span> <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># There is no limit to this length. Just be aware that it will consume memory.</span></span><br><span class="line"><span class="comment"># You can reclaim memory used by the slow log with SLOWLOG RESET.</span></span><br><span class="line"><span class="comment">#慢操作日志保留的最大条数</span></span><br><span class="line"><span class="string">slowlog-max-len</span> <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################ LATENCY MONITOR ##############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The Redis latency monitoring subsystem samples different operations</span></span><br><span class="line"><span class="comment"># at runtime in order to collect data related to possible sources of</span></span><br><span class="line"><span class="comment"># latency of a Redis instance.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Via the LATENCY command this information is available to the user that can</span></span><br><span class="line"><span class="comment"># print graphs and obtain reports.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The system only logs operations that were performed in a time equal or</span></span><br><span class="line"><span class="comment"># greater than the amount of milliseconds specified via the</span></span><br><span class="line"><span class="comment"># latency-monitor-threshold configuration directive. When its value is set</span></span><br><span class="line"><span class="comment"># to zero, the latency monitor is turned off.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># By default latency monitoring is disabled since it is mostly not needed</span></span><br><span class="line"><span class="comment"># if you don't have latency issues, and collecting data has a performance</span></span><br><span class="line"><span class="comment"># impact, that while very small, can be measured under big load. Latency</span></span><br><span class="line"><span class="comment"># monitoring can easily be enabled at runtime using the command</span></span><br><span class="line"><span class="comment"># "CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;" if needed.</span></span><br><span class="line"><span class="string">latency-monitor-threshold</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">############################# EVENT NOTIFICATION ##############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Redis can notify Pub/Sub clients about events happening in the key space.</span></span><br><span class="line"><span class="comment"># This feature is documented at http://redis.io/topics/notifications</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># For instance if keyspace events notification is enabled, and a client</span></span><br><span class="line"><span class="comment"># performs a DEL operation on key "foo" stored in the Database 0, two</span></span><br><span class="line"><span class="comment"># messages will be published via Pub/Sub:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># PUBLISH __keyspace@0__:foo del</span></span><br><span class="line"><span class="comment"># PUBLISH __keyevent@0__:del foo</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># It is possible to select the events that Redis will notify among a set</span></span><br><span class="line"><span class="comment"># of classes. Every class is identified by a single character:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  K     Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.</span></span><br><span class="line"><span class="comment">#  E     Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.</span></span><br><span class="line"><span class="comment">#  g     Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...</span></span><br><span class="line"><span class="comment">#  $     String commands</span></span><br><span class="line"><span class="comment">#  l     List commands</span></span><br><span class="line"><span class="comment">#  s     Set commands</span></span><br><span class="line"><span class="comment">#  h     Hash commands</span></span><br><span class="line"><span class="comment">#  z     Sorted set commands</span></span><br><span class="line"><span class="comment">#  x     Expired events (events generated every time a key expires)</span></span><br><span class="line"><span class="comment">#  e     Evicted events (events generated when a key is evicted for maxmemory)</span></span><br><span class="line"><span class="comment">#  t     Stream commands</span></span><br><span class="line"><span class="comment">#  m     Key-miss events (Note: It is not included in the 'A' class)</span></span><br><span class="line"><span class="comment">#  A     Alias for g$lshzxet, so that the "AKE" string means all the events</span></span><br><span class="line"><span class="comment">#        (Except key-miss events which are excluded from 'A' due to their</span></span><br><span class="line"><span class="comment">#         unique nature).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  The "notify-keyspace-events" takes as argument a string that is composed</span></span><br><span class="line"><span class="comment">#  of zero or multiple characters. The empty string means that notifications</span></span><br><span class="line"><span class="comment">#  are disabled.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  Example: to enable list and generic events, from the point of view of the</span></span><br><span class="line"><span class="comment">#           event name, use:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  notify-keyspace-events Elg</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  Example 2: to get the stream of the expired keys subscribing to channel</span></span><br><span class="line"><span class="comment">#             name __keyevent@0__:expired use:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  notify-keyspace-events Ex</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  By default all notifications are disabled because most users don't need</span></span><br><span class="line"><span class="comment">#  this feature and the feature has some overhead. Note that if you don't</span></span><br><span class="line"><span class="comment">#  specify at least one of K or E, no events will be delivered.</span></span><br><span class="line"><span class="comment">#键空间通知，""表示关闭</span></span><br><span class="line"><span class="string">notify-keyspace-events</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">############################### GOPHER SERVER #################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Redis contains an implementation of the Gopher protocol, as specified in</span></span><br><span class="line"><span class="comment"># the RFC 1436 (https://www.ietf.org/rfc/rfc1436.txt).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The Gopher protocol was very popular in the late '90s. It is an alternative</span></span><br><span class="line"><span class="comment"># to the web, and the implementation both server and client side is so simple</span></span><br><span class="line"><span class="comment"># that the Redis server has just 100 lines of code in order to implement this</span></span><br><span class="line"><span class="comment"># support.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># What do you do with Gopher nowadays? Well Gopher never *really* died, and</span></span><br><span class="line"><span class="comment"># lately there is a movement in order for the Gopher more hierarchical content</span></span><br><span class="line"><span class="comment"># composed of just plain text documents to be resurrected. Some want a simpler</span></span><br><span class="line"><span class="comment"># internet, others believe that the mainstream internet became too much</span></span><br><span class="line"><span class="comment"># controlled, and it's cool to create an alternative space for people that</span></span><br><span class="line"><span class="comment"># want a bit of fresh air.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Anyway for the 10nth birthday of the Redis, we gave it the Gopher protocol</span></span><br><span class="line"><span class="comment"># as a gift.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># --- HOW IT WORKS? ---</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The Redis Gopher support uses the inline protocol of Redis, and specifically</span></span><br><span class="line"><span class="comment"># two kind of inline requests that were anyway illegal: an empty request</span></span><br><span class="line"><span class="comment"># or any request that starts with "/" (there are no Redis commands starting</span></span><br><span class="line"><span class="comment"># with such a slash). Normal RESP2/RESP3 requests are completely out of the</span></span><br><span class="line"><span class="comment"># path of the Gopher protocol implementation and are served as usually as well.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># If you open a connection to Redis when Gopher is enabled and send it</span></span><br><span class="line"><span class="comment"># a string like "/foo", if there is a key named "/foo" it is served via the</span></span><br><span class="line"><span class="comment"># Gopher protocol.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># In order to create a real Gopher "hole" (the name of a Gopher site in Gopher</span></span><br><span class="line"><span class="comment"># talking), you likely need a script like the following:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   https://github.com/antirez/gopher2redis</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># --- SECURITY WARNING ---</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># If you plan to put Redis on the internet in a publicly accessible address</span></span><br><span class="line"><span class="comment"># to server Gopher pages MAKE SURE TO SET A PASSWORD to the instance.</span></span><br><span class="line"><span class="comment"># Once a password is set:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   1. The Gopher server (when enabled, not by default) will still serve</span></span><br><span class="line"><span class="comment">#      content via Gopher.</span></span><br><span class="line"><span class="comment">#   2. However other commands cannot be called before the client will</span></span><br><span class="line"><span class="comment">#      authenticate.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># So use the 'requirepass' option to protect your instance.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># To enable Gopher support uncomment the following line and set</span></span><br><span class="line"><span class="comment"># the option from no (the default) to yes.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># gopher-enabled no</span></span><br><span class="line"></span><br><span class="line"><span class="comment">############################### ADVANCED CONFIG ###############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hashes are encoded using a memory efficient data structure when they have a</span></span><br><span class="line"><span class="comment"># small number of entries, and the biggest entry does not exceed a given</span></span><br><span class="line"><span class="comment"># threshold. These thresholds can be configured using the following directives.</span></span><br><span class="line"><span class="comment">##ziplist中允许存储的最大条目个数</span></span><br><span class="line"><span class="string">hash-max-ziplist-entries</span> <span class="number">512</span></span><br><span class="line"><span class="comment">#ziplist中允许条目value值最大字节数</span></span><br><span class="line"><span class="string">hash-max-ziplist-value</span> <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Lists are also encoded in a special way to save a lot of space.</span></span><br><span class="line"><span class="comment"># The number of entries allowed per internal list node can be specified</span></span><br><span class="line"><span class="comment"># as a fixed maximum size or a maximum number of elements.</span></span><br><span class="line"><span class="comment"># For a fixed maximum size, use -5 through -1, meaning:</span></span><br><span class="line"><span class="comment"># -5: max size: 64 Kb  &lt;-- not recommended for normal workloads</span></span><br><span class="line"><span class="comment"># -4: max size: 32 Kb  &lt;-- not recommended</span></span><br><span class="line"><span class="comment"># -3: max size: 16 Kb  &lt;-- probably not recommended</span></span><br><span class="line"><span class="comment"># -2: max size: 8 Kb   &lt;-- good</span></span><br><span class="line"><span class="comment"># -1: max size: 4 Kb   &lt;-- good</span></span><br><span class="line"><span class="comment"># Positive numbers mean store up to _exactly_ that number of elements</span></span><br><span class="line"><span class="comment"># per list node.</span></span><br><span class="line"><span class="comment"># The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),</span></span><br><span class="line"><span class="comment"># but if your use case is unique, adjust the settings as necessary.</span></span><br><span class="line"><span class="string">list-max-ziplist-size</span> <span class="bullet">-2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Lists may also be compressed.</span></span><br><span class="line"><span class="comment"># Compress depth is the number of quicklist ziplist nodes from *each* side of</span></span><br><span class="line"><span class="comment"># the list to *exclude* from compression.  The head and tail of the list</span></span><br><span class="line"><span class="comment"># are always uncompressed for fast push/pop operations.  Settings are:</span></span><br><span class="line"><span class="comment"># 0: disable all list compression</span></span><br><span class="line"><span class="comment"># 1: depth 1 means "don't start compressing until after 1 node into the list,</span></span><br><span class="line"><span class="comment">#    going from either the head or tail"</span></span><br><span class="line"><span class="comment">#    So: [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]</span></span><br><span class="line"><span class="comment">#    [head], [tail] will always be uncompressed; inner nodes will compress.</span></span><br><span class="line"><span class="comment"># 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail]</span></span><br><span class="line"><span class="comment">#    2 here means: don't compress head or head-&gt;next or tail-&gt;prev or tail,</span></span><br><span class="line"><span class="comment">#    but compress all nodes between them.</span></span><br><span class="line"><span class="comment"># 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]</span></span><br><span class="line"><span class="comment"># etc.</span></span><br><span class="line"><span class="string">list-compress-depth</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sets have a special encoding in just one case: when a set is composed</span></span><br><span class="line"><span class="comment"># of just strings that happen to be integers in radix 10 in the range</span></span><br><span class="line"><span class="comment"># of 64 bit signed integers.</span></span><br><span class="line"><span class="comment"># The following configuration setting sets the limit in the size of the</span></span><br><span class="line"><span class="comment"># set in order to use this special memory saving encoding.</span></span><br><span class="line"><span class="comment">#intset中允许保存的最大条目个数,如果达到阀值,intset将会被重构为hashtable</span></span><br><span class="line"><span class="string">set-max-intset-entries</span> <span class="number">512</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Similarly to hashes and lists, sorted sets are also specially encoded in</span></span><br><span class="line"><span class="comment"># order to save a lot of space. This encoding is only used when the length and</span></span><br><span class="line"><span class="comment"># elements of a sorted set are below the following limits:</span></span><br><span class="line"><span class="comment">#设置同上</span></span><br><span class="line"><span class="string">zset-max-ziplist-entries</span> <span class="number">128</span></span><br><span class="line"><span class="string">zset-max-ziplist-value</span> <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># HyperLogLog sparse representation bytes limit. The limit includes the</span></span><br><span class="line"><span class="comment"># 16 bytes header. When an HyperLogLog using the sparse representation crosses</span></span><br><span class="line"><span class="comment"># this limit, it is converted into the dense representation.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># A value greater than 16000 is totally useless, since at that point the</span></span><br><span class="line"><span class="comment"># dense representation is more memory efficient.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The suggested value is ~ 3000 in order to have the benefits of</span></span><br><span class="line"><span class="comment"># the space efficient encoding without slowing down too much PFADD,</span></span><br><span class="line"><span class="comment"># which is O(N) with the sparse encoding. The value can be raised to</span></span><br><span class="line"><span class="comment"># ~ 10000 when CPU is not a concern, but space is, and the data set is</span></span><br><span class="line"><span class="comment"># composed of many HyperLogLogs with cardinality in the 0 - 15000 range.</span></span><br><span class="line"><span class="string">hll-sparse-max-bytes</span> <span class="number">3000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Streams macro node max size / items. The stream data structure is a radix</span></span><br><span class="line"><span class="comment"># tree of big nodes that encode multiple items inside. Using this configuration</span></span><br><span class="line"><span class="comment"># it is possible to configure how big a single node can be in bytes, and the</span></span><br><span class="line"><span class="comment"># maximum number of items it may contain before switching to a new node when</span></span><br><span class="line"><span class="comment"># appending new stream entries. If any of the following settings are set to</span></span><br><span class="line"><span class="comment"># zero, the limit is ignored, so for instance it is possible to set just a</span></span><br><span class="line"><span class="comment"># max entires limit by setting max-bytes to 0 and max-entries to the desired</span></span><br><span class="line"><span class="comment"># value.</span></span><br><span class="line"><span class="string">stream-node-max-bytes</span> <span class="number">4096</span></span><br><span class="line"><span class="string">stream-node-max-entries</span> <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in</span></span><br><span class="line"><span class="comment"># order to help rehashing the main Redis hash table (the one mapping top-level</span></span><br><span class="line"><span class="comment"># keys to values). The hash table implementation Redis uses (see dict.c)</span></span><br><span class="line"><span class="comment"># performs a lazy rehashing: the more operation you run into a hash table</span></span><br><span class="line"><span class="comment"># that is rehashing, the more rehashing "steps" are performed, so if the</span></span><br><span class="line"><span class="comment"># server is idle the rehashing is never complete and some more memory is used</span></span><br><span class="line"><span class="comment"># by the hash table.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The default is to use this millisecond 10 times every second in order to</span></span><br><span class="line"><span class="comment"># actively rehash the main dictionaries, freeing memory when possible.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># If unsure:</span></span><br><span class="line"><span class="comment"># use "activerehashing no" if you have hard latency requirements and it is</span></span><br><span class="line"><span class="comment"># not a good thing in your environment that Redis can reply from time to time</span></span><br><span class="line"><span class="comment"># to queries with 2 milliseconds delay.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># use "activerehashing yes" if you don't have such hard requirements but</span></span><br><span class="line"><span class="comment"># want to free memory asap when possible.</span></span><br><span class="line"><span class="comment">#是否开启顶层数据结构的rehash功能,如果内存允许,请开启</span></span><br><span class="line"><span class="string">activerehashing</span> <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The client output buffer limits can be used to force disconnection of clients</span></span><br><span class="line"><span class="comment"># that are not reading data from the server fast enough for some reason (a</span></span><br><span class="line"><span class="comment"># common reason is that a Pub/Sub client can't consume messages as fast as the</span></span><br><span class="line"><span class="comment"># publisher can produce them).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The limit can be set differently for the three different classes of clients:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># normal -&gt; normal clients including MONITOR clients</span></span><br><span class="line"><span class="comment"># replica  -&gt; replica clients</span></span><br><span class="line"><span class="comment"># pubsub -&gt; clients subscribed to at least one pubsub channel or pattern</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The syntax of every client-output-buffer-limit directive is the following:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># A client is immediately disconnected once the hard limit is reached, or if</span></span><br><span class="line"><span class="comment"># the soft limit is reached and remains reached for the specified number of</span></span><br><span class="line"><span class="comment"># seconds (continuously).</span></span><br><span class="line"><span class="comment"># So for instance if the hard limit is 32 megabytes and the soft limit is</span></span><br><span class="line"><span class="comment"># 16 megabytes / 10 seconds, the client will get disconnected immediately</span></span><br><span class="line"><span class="comment"># if the size of the output buffers reach 32 megabytes, but will also get</span></span><br><span class="line"><span class="comment"># disconnected if the client reaches 16 megabytes and continuously overcomes</span></span><br><span class="line"><span class="comment"># the limit for 10 seconds.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># By default normal clients are not limited because they don't receive data</span></span><br><span class="line"><span class="comment"># without asking (in a push way), but just after a request, so only</span></span><br><span class="line"><span class="comment"># asynchronous clients may create a scenario where data is requested faster</span></span><br><span class="line"><span class="comment"># than it can read.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Instead there is a default limit for pubsub and replica clients, since</span></span><br><span class="line"><span class="comment"># subscribers and replicas receive data in a push fashion.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Both the hard or the soft limit can be disabled by setting them to zero.</span></span><br><span class="line"><span class="comment">#客户端buffer控制</span></span><br><span class="line"><span class="string">client-output-buffer-limit</span> <span class="string">normal</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"><span class="string">client-output-buffer-limit</span> <span class="string">replica</span> <span class="number">256</span><span class="string">mb</span> <span class="number">64</span><span class="string">mb</span> <span class="number">60</span></span><br><span class="line"><span class="string">client-output-buffer-limit</span> <span class="string">pubsub</span> <span class="number">32</span><span class="string">mb</span> <span class="number">8</span><span class="string">mb</span> <span class="number">60</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Client query buffers accumulate new commands. They are limited to a fixed</span></span><br><span class="line"><span class="comment"># amount by default in order to avoid that a protocol desynchronization (for</span></span><br><span class="line"><span class="comment"># instance due to a bug in the client) will lead to unbound memory usage in</span></span><br><span class="line"><span class="comment"># the query buffer. However you can configure it here if you have very special</span></span><br><span class="line"><span class="comment"># needs, such us huge multi/exec requests or alike.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># client-query-buffer-limit 1gb</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In the Redis protocol, bulk requests, that are, elements representing single</span></span><br><span class="line"><span class="comment"># strings, are normally limited ot 512 mb. However you can change this limit</span></span><br><span class="line"><span class="comment"># here.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># proto-max-bulk-len 512mb</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Redis calls an internal function to perform many background tasks, like</span></span><br><span class="line"><span class="comment"># closing connections of clients in timeout, purging expired keys that are</span></span><br><span class="line"><span class="comment"># never requested, and so forth.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Not all tasks are performed with the same frequency, but Redis checks for</span></span><br><span class="line"><span class="comment"># tasks to perform according to the specified "hz" value.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># By default "hz" is set to 10. Raising the value will use more CPU when</span></span><br><span class="line"><span class="comment"># Redis is idle, but at the same time will make Redis more responsive when</span></span><br><span class="line"><span class="comment"># there are many keys expiring at the same time, and timeouts may be</span></span><br><span class="line"><span class="comment"># handled with more precision.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The range is between 1 and 500, however a value over 100 is usually not</span></span><br><span class="line"><span class="comment"># a good idea. Most users should use the default of 10 and raise this up to</span></span><br><span class="line"><span class="comment"># 100 only in environments where very low latency is required.</span></span><br><span class="line"><span class="comment">#Redis server执行后台任务的频率,默认为10,此值越大表示redis对"间歇性task"的执行次数越频繁</span></span><br><span class="line"><span class="string">hz</span> <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Normally it is useful to have an HZ value which is proportional to the</span></span><br><span class="line"><span class="comment"># number of clients connected. This is useful in order, for instance, to</span></span><br><span class="line"><span class="comment"># avoid too many clients are processed for each background task invocation</span></span><br><span class="line"><span class="comment"># in order to avoid latency spikes.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Since the default HZ value by default is conservatively set to 10, Redis</span></span><br><span class="line"><span class="comment"># offers, and enables by default, the ability to use an adaptive HZ value</span></span><br><span class="line"><span class="comment"># which will temporary raise when there are many connected clients.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># When dynamic HZ is enabled, the actual configured HZ will be used</span></span><br><span class="line"><span class="comment"># as a baseline, but multiples of the configured HZ value will be actually</span></span><br><span class="line"><span class="comment"># used as needed once more clients are connected. In this way an idle</span></span><br><span class="line"><span class="comment"># instance will use very little CPU time while a busy instance will be</span></span><br><span class="line"><span class="comment"># more responsive.</span></span><br><span class="line"><span class="string">dynamic-hz</span> <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># When a child rewrites the AOF file, if the following option is enabled</span></span><br><span class="line"><span class="comment"># the file will be fsync-ed every 32 MB of data generated. This is useful</span></span><br><span class="line"><span class="comment"># in order to commit the file to the disk more incrementally and avoid</span></span><br><span class="line"><span class="comment"># big latency spikes.</span></span><br><span class="line"><span class="comment">#aof rewrite过程中,是否采取增量"文件同步"策略,默认为"yes",而且必须为yes</span></span><br><span class="line"><span class="string">aof-rewrite-incremental-fsync</span> <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># When redis saves RDB file, if the following option is enabled</span></span><br><span class="line"><span class="comment"># the file will be fsync-ed every 32 MB of data generated. This is useful</span></span><br><span class="line"><span class="comment"># in order to commit the file to the disk more incrementally and avoid</span></span><br><span class="line"><span class="comment"># big latency spikes.</span></span><br><span class="line"><span class="string">rdb-save-incremental-fsync</span> <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good</span></span><br><span class="line"><span class="comment"># idea to start with the default settings and only change them after investigating</span></span><br><span class="line"><span class="comment"># how to improve the performances and how the keys LFU change over time, which</span></span><br><span class="line"><span class="comment"># is possible to inspect via the OBJECT FREQ command.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># There are two tunable parameters in the Redis LFU implementation: the</span></span><br><span class="line"><span class="comment"># counter logarithm factor and the counter decay time. It is important to</span></span><br><span class="line"><span class="comment"># understand what the two parameters mean before changing them.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis</span></span><br><span class="line"><span class="comment"># uses a probabilistic increment with logarithmic behavior. Given the value</span></span><br><span class="line"><span class="comment"># of the old counter, when a key is accessed, the counter is incremented in</span></span><br><span class="line"><span class="comment"># this way:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 1. A random number R between 0 and 1 is extracted.</span></span><br><span class="line"><span class="comment"># 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).</span></span><br><span class="line"><span class="comment"># 3. The counter is incremented only if R &lt; P.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The default lfu-log-factor is 10. This is a table of how the frequency</span></span><br><span class="line"><span class="comment"># counter changes with a different number of accesses with different</span></span><br><span class="line"><span class="comment"># logarithmic factors:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># +--------+------------+------------+------------+------------+------------+</span></span><br><span class="line"><span class="comment"># | factor | 100 hits   | 1000 hits  | 100K hits  | 1M hits    | 10M hits   |</span></span><br><span class="line"><span class="comment"># +--------+------------+------------+------------+------------+------------+</span></span><br><span class="line"><span class="comment"># | 0      | 104        | 255        | 255        | 255        | 255        |</span></span><br><span class="line"><span class="comment"># +--------+------------+------------+------------+------------+------------+</span></span><br><span class="line"><span class="comment"># | 1      | 18         | 49         | 255        | 255        | 255        |</span></span><br><span class="line"><span class="comment"># +--------+------------+------------+------------+------------+------------+</span></span><br><span class="line"><span class="comment"># | 10     | 10         | 18         | 142        | 255        | 255        |</span></span><br><span class="line"><span class="comment"># +--------+------------+------------+------------+------------+------------+</span></span><br><span class="line"><span class="comment"># | 100    | 8          | 11         | 49         | 143        | 255        |</span></span><br><span class="line"><span class="comment"># +--------+------------+------------+------------+------------+------------+</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The above table was obtained by running the following commands:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   redis-benchmark -n 1000000 incr foo</span></span><br><span class="line"><span class="comment">#   redis-cli object freq foo</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># NOTE 2: The counter initial value is 5 in order to give new objects a chance</span></span><br><span class="line"><span class="comment"># to accumulate hits.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The counter decay time is the time, in minutes, that must elapse in order</span></span><br><span class="line"><span class="comment"># for the key counter to be divided by two (or decremented if it has a value</span></span><br><span class="line"><span class="comment"># less &lt;= 10).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The default value for the lfu-decay-time is 1. A Special value of 0 means to</span></span><br><span class="line"><span class="comment"># decay the counter every time it happens to be scanned.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># lfu-log-factor 10</span></span><br><span class="line"><span class="comment"># lfu-decay-time 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">########################### ACTIVE DEFRAGMENTATION #######################</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># What is active defragmentation?</span></span><br><span class="line"><span class="comment"># -------------------------------</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Active (online) defragmentation allows a Redis server to compact the</span></span><br><span class="line"><span class="comment"># spaces left between small allocations and deallocations of data in memory,</span></span><br><span class="line"><span class="comment"># thus allowing to reclaim back memory.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Fragmentation is a natural process that happens with every allocator (but</span></span><br><span class="line"><span class="comment"># less so with Jemalloc, fortunately) and certain workloads. Normally a server</span></span><br><span class="line"><span class="comment"># restart is needed in order to lower the fragmentation, or at least to flush</span></span><br><span class="line"><span class="comment"># away all the data and create it again. However thanks to this feature</span></span><br><span class="line"><span class="comment"># implemented by Oran Agra for Redis 4.0 this process can happen at runtime</span></span><br><span class="line"><span class="comment"># in an "hot" way, while the server is running.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Basically when the fragmentation is over a certain level (see the</span></span><br><span class="line"><span class="comment"># configuration options below) Redis will start to create new copies of the</span></span><br><span class="line"><span class="comment"># values in contiguous memory regions by exploiting certain specific Jemalloc</span></span><br><span class="line"><span class="comment"># features (in order to understand if an allocation is causing fragmentation</span></span><br><span class="line"><span class="comment"># and to allocate it in a better place), and at the same time, will release the</span></span><br><span class="line"><span class="comment"># old copies of the data. This process, repeated incrementally for all the keys</span></span><br><span class="line"><span class="comment"># will cause the fragmentation to drop back to normal values.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Important things to understand:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 1. This feature is disabled by default, and only works if you compiled Redis</span></span><br><span class="line"><span class="comment">#    to use the copy of Jemalloc we ship with the source code of Redis.</span></span><br><span class="line"><span class="comment">#    This is the default with Linux builds.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 2. You never need to enable this feature if you don't have fragmentation</span></span><br><span class="line"><span class="comment">#    issues.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 3. Once you experience fragmentation, you can enable this feature when</span></span><br><span class="line"><span class="comment">#    needed with the command "CONFIG SET activedefrag yes".</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The configuration parameters are able to fine tune the behavior of the</span></span><br><span class="line"><span class="comment"># defragmentation process. If you are not sure about what they mean it is</span></span><br><span class="line"><span class="comment"># a good idea to leave the defaults untouched.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enabled active defragmentation</span></span><br><span class="line"><span class="comment"># activedefrag no</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Minimum amount of fragmentation waste to start active defrag</span></span><br><span class="line"><span class="comment"># active-defrag-ignore-bytes 100mb</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Minimum percentage of fragmentation to start active defrag</span></span><br><span class="line"><span class="comment"># active-defrag-threshold-lower 10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Maximum percentage of fragmentation at which we use maximum effort</span></span><br><span class="line"><span class="comment"># active-defrag-threshold-upper 100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Minimal effort for defrag in CPU percentage, to be used when the lower</span></span><br><span class="line"><span class="comment"># threshold is reached</span></span><br><span class="line"><span class="comment"># active-defrag-cycle-min 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Maximal effort for defrag in CPU percentage, to be used when the upper</span></span><br><span class="line"><span class="comment"># threshold is reached</span></span><br><span class="line"><span class="comment"># active-defrag-cycle-max 25</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Maximum number of set/hash/zset/list fields that will be processed from</span></span><br><span class="line"><span class="comment"># the main dictionary scan</span></span><br><span class="line"><span class="comment"># active-defrag-max-scan-fields 1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Jemalloc background thread for purging will be enabled by default</span></span><br><span class="line"><span class="string">jemalloc-bg-thread</span> <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># It is possible to pin different threads and processes of Redis to specific</span></span><br><span class="line"><span class="comment"># CPUs in your system, in order to maximize the performances of the server.</span></span><br><span class="line"><span class="comment"># This is useful both in order to pin different Redis threads in different</span></span><br><span class="line"><span class="comment"># CPUs, but also in order to make sure that multiple Redis instances running</span></span><br><span class="line"><span class="comment"># in the same host will be pinned to different CPUs.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Normally you can do this using the "taskset" command, however it is also</span></span><br><span class="line"><span class="comment"># possible to this via Redis configuration directly, both in Linux and FreeBSD.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># You can pin the server/IO threads, bio threads, aof rewrite child process, and</span></span><br><span class="line"><span class="comment"># the bgsave child process. The syntax to specify the cpu list is the same as</span></span><br><span class="line"><span class="comment"># the taskset command:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Set redis server/io threads to cpu affinity 0,2,4,6:</span></span><br><span class="line"><span class="comment"># server_cpulist 0-7:2</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Set bio threads to cpu affinity 1,3:</span></span><br><span class="line"><span class="comment"># bio_cpulist 1,3</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Set aof rewrite child process to cpu affinity 8,9,10,11:</span></span><br><span class="line"><span class="comment"># aof_rewrite_cpulist 8-11</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Set bgsave child process to cpu affinity 1,10,11</span></span><br><span class="line"><span class="comment"># bgsave_cpulist 1,10-11</span></span><br></pre></td></tr></table></figure>
<h3 id="上传本地egg服务端代码到服务器"><a href="#上传本地egg服务端代码到服务器" class="headerlink" title="上传本地egg服务端代码到服务器"></a>上传本地egg服务端代码到服务器</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp -rp egg.zip root@43.138.12.18:/home</span><br></pre></td></tr></table></figure>
<p><img src="https://s.poetries.work/uploads/2022/06/0327d670f6d365a3.png" alt></p>
<p><strong>解压</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">unzip -u -d server egg.zip</span><br></pre></td></tr></table></figure>
<p><img src="https://s.poetries.work/uploads/2022/06/1b6745e93aa9117b.png" alt></p>
<h3 id="启动egg服务"><a href="#启动egg服务" class="headerlink" title="启动egg服务"></a>启动egg服务</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cd egg</span><br><span class="line"># -d 后台方式启动</span><br><span class="line"></span><br><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure>
<p><img src="https://s.poetries.work/uploads/2022/06/bc579093039d5dff.png" alt><br><img src="https://s.poetries.work/uploads/2022/06/e0867be8b406e212.png" alt><br><img src="https://s.poetries.work/uploads/2022/06/b2cacc63e98f5a06.png" alt></p>
<h3 id="测试服务"><a href="#测试服务" class="headerlink" title="测试服务"></a>测试服务</h3><p><strong>vscode本地连接线上数据库测试</strong></p>
<p><img src="https://s.poetries.work/uploads/2022/06/1f3bfb1ae436d23a.png" alt></p>
<p><strong>redis服务连接测试</strong></p>
<p>无需密码登录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">redis-cli -h 43.23.121.12 -p 6380</span><br></pre></td></tr></table></figure>
<p><img src="https://s.poetries.work/uploads/2022/06/eda7643690d38f9d.png" alt><br><img src="https://s.poetries.work/uploads/2022/06/52c4f5f008df9f52.png" alt></p>
<p>设置密码后的登录方式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">redis-cli -h 43.31.121.12 -p 6380</span><br><span class="line"></span><br><span class="line">keys *</span><br><span class="line">auth [username] password</span><br></pre></td></tr></table></figure>
<p><img src="https://s.poetries.work/uploads/2022/06/ca3f547b33538895.png" alt></p>
<blockquote>
<p>缓存服务测试</p>
</blockquote>
<p><img src="https://s.poetries.work/uploads/2022/06/575fadaac0763dc6.png" alt><br><img src="https://s.poetries.work/uploads/2022/06/54f7c259eaf6172c.png" alt></p>
<p><strong>测试egg接口</strong></p>
<p><img src="https://s.poetries.work/uploads/2022/06/dc8e99cf5574f3fd.png" alt></p>
<p><strong>访问前端项目测试接口</strong></p>
<p><img src="https://s.poetries.work/uploads/2022/06/64b59c23ed0cc0d7.png" alt></p>
<h1 id="五、部署到云托管"><a href="#五、部署到云托管" class="headerlink" title="五、部署到云托管"></a>五、部署到云托管</h1><blockquote>
<p>云托管流水线部署更方便</p>
</blockquote>
<h2 id="5-1-redis服务"><a href="#5-1-redis服务" class="headerlink" title="5.1 redis服务"></a>5.1 redis服务</h2><p>这里我们上面部署使用的自建服务器上docker搭建的redis服务作为演示</p>
<p><img src="https://s.poetries.work/uploads/2022/06/c74e6e89043a20d6.png" alt></p>
<h2 id="5-2-mysql服务"><a href="#5-2-mysql服务" class="headerlink" title="5.2 mysql服务"></a>5.2 mysql服务</h2><p>这里我们上面部署使用的自建服务器上docker搭建的mysql服务作为演示</p>
<p><img src="https://s.poetries.work/uploads/2022/06/1e17df55d9c3634f.png" alt></p>
<h2 id="5-3-egg部署"><a href="#5-3-egg部署" class="headerlink" title="5.3 egg部署"></a>5.3 egg部署</h2><h3 id="修改代码"><a href="#修改代码" class="headerlink" title="修改代码"></a>修改代码</h3><p><img src="https://s.poetries.work/uploads/2022/06/144606d89bc20359.png" alt></p>
<p>然后上传代码到github，通过云托管流水线构建</p>
<h3 id="新建服务"><a href="#新建服务" class="headerlink" title="新建服务"></a>新建服务</h3><p><img src="https://s.poetries.work/uploads/2022/06/c09974e946ab3075.png" alt></p>
<p><img src="https://s.poetries.work/uploads/2022/06/71c1f38043c67c1b.png" alt></p>
<p>点击发布后，云托管会执行Dockerfile构建流水线，到日志可以查看构建进度</p>
<p><img src="https://s.poetries.work/uploads/2022/06/2c6ce688f421ef04.png" alt></p>
<p><img src="https://s.poetries.work/uploads/2022/06/0f3919c9924b6071.png" alt></p>
<p><strong>微信云托管部署成功后，可以在实例列表，点击进入容器看到代码</strong>，这里里面的内容不能修改，在容器启动后会覆盖</p>
<p><img src="https://s.poetries.work/uploads/2022/06/3e1b91db0e2b0718.png" alt><br><img src="https://s.poetries.work/uploads/2022/06/d0dd17b85fd1e305.png" alt></p>
<h3 id="调试接口"><a href="#调试接口" class="headerlink" title="调试接口"></a>调试接口</h3><p><img src="https://s.poetries.work/uploads/2022/06/e3a9c648993208d2.png" alt></p>
<p><strong>postman测试</strong></p>
<p><img src="https://s.poetries.work/uploads/2022/06/713c1e6256b93b9a.png" alt></p>
<p><strong>测试redis服务</strong></p>
<p><img src="https://s.poetries.work/uploads/2022/06/c3b97b3e633cb928.png" alt><br><img src="https://s.poetries.work/uploads/2022/06/91202ab620ae6b65.png" alt></p>
<p>至此部署到微信云托管完成，后续修改代码提交到github会自动触发云托管部署</p>
<h1 id="六、egg部署到腾讯serverless"><a href="#六、egg部署到腾讯serverless" class="headerlink" title="六、egg部署到腾讯serverless"></a>六、egg部署到腾讯serverless</h1><p>需要注意，云函数的代码包不能超过500M</p>
<p><img src="https://s.poetries.work/uploads/2022/06/41b9fcbf9242921f.png" alt></p>
<h2 id="6-1-修改egg配置"><a href="#6-1-修改egg配置" class="headerlink" title="6.1 修改egg配置"></a>6.1 修改egg配置</h2><blockquote>
<p>由于云函数在执行时，只有 <code>/tmp</code> 可读写的，所以我们需要将 egg.js 框架运行尝试的日志写到该目录下，为此需要修改 <code>config/config.default.js</code> 中的配置如下：</p>
</blockquote>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> config = (exports = &#123;</span><br><span class="line">  env: <span class="string">"prod"</span>, <span class="comment">// 推荐云函数的 egg 运行环境变量修改为 prod</span></span><br><span class="line">  rundir: <span class="string">'/tmp'</span>,</span><br><span class="line">  logger: &#123;</span><br><span class="line">    dir: <span class="string">'/tmp'</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="6-2-命令行部署"><a href="#6-2-命令行部署" class="headerlink" title="6.2 命令行部署"></a>6.2 命令行部署</h2><figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 安装Serverless 框架</span></span><br><span class="line">npm i -g serverless</span><br></pre></td></tr></table></figure>
<h3 id="配置-YAML"><a href="#配置-YAML" class="headerlink" title="配置 YAML"></a>配置 YAML</h3><p><strong>在 egg 项目根目录,新建 Serverless 配置文件 serverless.yml</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">app: appDemo</span><br><span class="line">stage: dev</span><br><span class="line">component: egg</span><br><span class="line">name: eggDemo</span><br><span class="line"></span><br><span class="line">inputs:</span><br><span class="line">  src: ./src</span><br><span class="line">  region: ap-guangzhou</span><br><span class="line">  functionName: eggDemo</span><br><span class="line">  runtime: Nodejs10.15</span><br><span class="line">  apigatewayConf:</span><br><span class="line">    protocols:</span><br><span class="line">      - http</span><br><span class="line">      - https</span><br><span class="line">    environment: release</span><br></pre></td></tr></table></figure>
<h3 id="部署到腾讯云"><a href="#部署到腾讯云" class="headerlink" title="部署到腾讯云"></a>部署到腾讯云</h3><ul>
<li>部署命令： <code>sls deploy</code>(意: <code>sls</code> 是 <code>serverless</code> 命令的简写。)</li>
<li>更多配置参考 <a href="https://github.com/serverless-components/tencent-egg/tree/v2" target="_blank" rel="noopener">https://github.com/serverless-components/tencent-egg/tree/v2</a></li>
</ul>
<p><img src="https://s.poetries.work/uploads/2022/06/bdea8f2beb76abc4.png" alt><br><img src="https://s.poetries.work/uploads/2022/06/471ac2511396e898.png" alt><br><img src="https://s.poetries.work/uploads/2022/06/51a5bdc6a3e2ee10.png" alt></p>
<h3 id="移除"><a href="#移除" class="headerlink" title="移除"></a>移除</h3><p>通过以下命令移除部署的 Egg 服务资源，包括云函数和 API 网关。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sls remove</span><br></pre></td></tr></table></figure>
<h3 id="账号配置（可选）"><a href="#账号配置（可选）" class="headerlink" title="账号配置（可选）"></a>账号配置（可选）</h3><blockquote>
<p>当前默认支持 CLI 扫描二维码登录，如您希望配置持久的环境变量/秘钥信息，也可以在项目根目录 <code>serverless-egg</code> 中创建 <code>.env</code> 文件：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># .env</span><br><span class="line">TENCENT_SECRET_ID=XXX</span><br><span class="line">TENCENT_SECRET_KEY=XXX</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果已有腾讯云账号，可以在 API <a href="https://console.cloud.tencent.com/cam/capi" target="_blank" rel="noopener">密钥管理</a> 中获取 SecretId 和SecretKey.</p>
</blockquote>
<h3 id="注意！！！"><a href="#注意！！！" class="headerlink" title="注意！！！"></a>注意！！！</h3><blockquote>
<p>通常初始化的 egg 项目，会自动创建 <code>app/public</code> 目录。但是在打包压缩时，如果该目录为空，则部署后，该目录不会存在。所以 egg 项目启动时会自动创建，但是云函数是没有操作权限的，建议可以在 <code>app/public</code> 目录下创建一个空文件 <code>.gitkeep</code>，来解决此问题。</p>
</blockquote>
<h2 id="6-3-控制台创建部署-模板部署"><a href="#6-3-控制台创建部署-模板部署" class="headerlink" title="6.3 控制台创建部署-模板部署"></a>6.3 控制台创建部署-模板部署</h2><ol>
<li>登录控制台 <a href="https://console.cloud.tencent.com/sls" target="_blank" rel="noopener">https://console.cloud.tencent.com/sls</a></li>
<li>单击新建应用，选择Web 应用&gt;Egg 框架，如下图所示：</li>
</ol>
<p><img src="https://s.poetries.work/uploads/2022/06/fbf0b03c839cd73e.png" alt></p>
<ol start="3">
<li>单击“下一步”，完成基础配置选择。</li>
</ol>
<p><img src="https://s.poetries.work/uploads/2022/06/58b123659a00b194.png" alt></p>
<ol start="4">
<li>上传方式，选择示例代码直接部署，单击完成，即可开始应用的部署。</li>
<li>部署完成后，您可在应用详情页面，查看示例应用的基本信息，并通过 API 网关生成的访问路径 URL 进行访问，查看您部署的 Egg 项目。</li>
</ol>
<p><img src="https://s.poetries.work/uploads/2022/06/c4c3e330b1a6af68.png" alt></p>
<h2 id="6-4-控制台创建部署-自定义部署"><a href="#6-4-控制台创建部署-自定义部署" class="headerlink" title="6.4 控制台创建部署-自定义部署"></a>6.4 控制台创建部署-自定义部署</h2><blockquote>
<p>如果除了代码部署外，您还需要更多能力或资源创建，如自动创建层托管依赖、一键实现静态资源分离、支持代码仓库直接拉取等，可以通过应用控制台，完成 Web 应用的创建工作</p>
</blockquote>
<h3 id="初始化项目"><a href="#初始化项目" class="headerlink" title="初始化项目"></a>初始化项目</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir egg-example &amp;&amp; cd egg-example</span><br><span class="line">npm init egg --type=simple</span><br><span class="line">npm i</span><br></pre></td></tr></table></figure>
<h3 id="部署上云"><a href="#部署上云" class="headerlink" title="部署上云"></a>部署上云</h3><blockquote>
<p>接下来执行以下步骤，对本地已创建完成的项目进行简单修改，使其可以通过 Web Function 快速部署，对于 Egg 框架，具体改造说明如下：</p>
</blockquote>
<ul>
<li>修改监听地址与端口为 <code>0.0.0.0:9000</code>。</li>
<li>修改写入路径，serverless 环境下只有 <code>/tmp</code> 目录可读写。</li>
<li>新增 <code>scf_bootstrap</code> 启动文件。</li>
</ul>
<p><strong>1. (可选)配置 scf_bootstrap 启动文件</strong></p>
<p>您也可以在控制台完成该模块配置。</p>
<p><img src="https://s.poetries.work/uploads/2022/06/85b3d73ea922f0ea.png" alt></p>
<blockquote>
<p>在项目根目录下新建 <code>scf_bootstrap</code> 启动文件，在该文件添加如下内容（用于配置环境变量和启动服务，此处仅为示例，具体操作请以您实际业务场景来调整）：</p>
</blockquote>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/var/lang/node12/bin/node</span></span><br><span class="line"></span><br><span class="line"><span class="meta">'use strict'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * docker 中 node 路径：/var/lang/node12/bin/node</span></span><br><span class="line"><span class="comment"> * 由于 serverless 函数只有 /tmp 读写权限，所以在启动时需要修改两个环境变量</span></span><br><span class="line"><span class="comment"> * NODE_LOG_DIR 是为了改写 egg-scripts 默认 node 写入路径（~/logs）-&gt; /tmp</span></span><br><span class="line"><span class="comment"> * EGG_APP_CONFIG 是为了修改 egg 应有的默认当前目录 -&gt; /tmp</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line">process.env.EGG_SERVER_ENV = <span class="string">'prod'</span>;</span><br><span class="line">process.env.NODE_ENV = <span class="string">'production'</span>;</span><br><span class="line">process.env.NODE_LOG_DIR = <span class="string">'/tmp'</span>;</span><br><span class="line">process.env.EGG_APP_CONFIG = <span class="string">'&#123;"rundir":"/tmp","logger":&#123;"dir":"/tmp"&#125;&#125;'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> &#123; Application &#125; = <span class="built_in">require</span>(<span class="string">'egg'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果通过层部署 node_modules 就需要修改 eggPath</span></span><br><span class="line"><span class="built_in">Object</span>.defineProperty(Application.prototype, <span class="built_in">Symbol</span>.for(<span class="string">'egg#eggPath'</span>), &#123;</span><br><span class="line">  value: <span class="string">'/opt'</span>,</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> app = <span class="keyword">new</span> Application(&#123;</span><br><span class="line">  mode: <span class="string">'single'</span>,</span><br><span class="line">  env: <span class="string">'prod'</span>,</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">app.listen(<span class="number">9000</span>, <span class="string">'0.0.0.0'</span>, () =&gt; &#123;</span><br><span class="line">  <span class="built_in">console</span>.log(<span class="string">'Server start on http://0.0.0.0:9000'</span>);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>新建完成后，还需执行以下命令修改文件可执行权限，默认需要 777 或 755 权限才可正常启动。示例如下：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod 777 scf_bootstrap</span><br></pre></td></tr></table></figure>
<p><strong>2. 控制台上传</strong></p>
<blockquote>
<p>您可以在控制台完成启动文件 scf_bootstrap 内容配置，配置完成后，控制台将为您自动生成 启动文件，和项目代码一起打包部署</p>
</blockquote>
<p>启动文件以项目内文件为准，如果您的项目里已经包含 <code>scf_bootstrap</code> 文件，将不会覆盖该内容。</p>
<p><img src="https://s.poetries.work/uploads/2022/06/a96ce1c3799f5951.png" alt><br><img src="https://s.poetries.work/uploads/2022/06/ce48d9dd73af824f.png" alt><br><img src="https://s.poetries.work/uploads/2022/06/9a0e3dfa25a6ed3d.png" alt></p>
<p><img src="https://s.poetries.work/uploads/2022/06/e9dd1225bf0d37cd.png" alt></p>
<p>查看函数，修改代码查看日志等</p>
<p><img src="https://s.poetries.work/uploads/2022/06/ca238c35541dd8df.png" alt></p>
<p><strong>高级配置管理</strong></p>
<blockquote>
<p>您可在“高级配置”里进行更多应用管理操作，如创建层、绑定自定义域名、配置环境变量等。</p>
</blockquote>
<p><img src="https://s.poetries.work/uploads/2022/06/4ab13bbb74bcdf89.png" alt></p>
<h2 id="6-5-测试接口"><a href="#6-5-测试接口" class="headerlink" title="6.5 测试接口"></a>6.5 测试接口</h2><p><img src="https://s.poetries.work/uploads/2022/06/5a4f88d78a4740fb.png" alt><br><img src="https://s.poetries.work/uploads/2022/06/692ad2fc22a7721d.png" alt><br><img src="https://s.poetries.work/uploads/2022/06/38120fb475238537.png" alt></p>

      </div>
    
  </div>

</article>

<!-- <button class="assist-btn2 circle" id="assist_btn2" title="点亮屏幕" style="left: 27px; top: 152px;">
  <i class="iconfont" style="display:inline-block;color:red;width:20px;height:20px;">&#xe61d;</i>
</button>
<button class="assist-btn1 circle" id="assist_btn1" title="关闭屏幕亮度" style="left: 27px; top: 152px;">
  <i class="iconfont toc-title" style="display:inline-block;color:red;width:20px;height:20px;">&#xe61d;</i>
</button> -->


<script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>	

<script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script>
<script>
  function getCookie(key) {
    if (document.cookie.length > 0) {
      var start = document.cookie.indexOf(key + "=");
      if (start !== -1) {
        start = start + key.length + 1;
        var end = document.cookie.indexOf(";", start);
        if (end === -1) end = document.cookie.length;
        return unescape(document.cookie.substring(start, end));
      }
    }
    return "";
  }
  const feToken = getCookie('fe-token');
  const btw = new BTWPlugin();
  console.log('ft', feToken)
  if(!feToken) {
    btw.init({
      id: "container",
      blogId: "22699-1592137983091-414",
      name: "前端进阶之旅",
      qrcode: "https://poetries1.gitee.io/img-repo/2020/06/qrcode.jpg",
      keyword: "3a3b3c",
    });
  }
</script>

<script type="text/javascript">

// white theme
var body = {color: "#555", background: "#000"};
var a_tag = {color: "#222"};
var header = { background: "#222"};
var logo_line_i = {background: "#222"};
// var post_code = {background: "#eee", color: "#222"};

function switch_theme() {
 $("body").css(body);
 $("a:not('.links-of-author-item a, .site-state-item a, .site-state-posts a, .feed-link a, .motion-element a, .post-tags a, .show-commit-cls a, #donate_board a')").css(a_tag);
 $(".header, .footer").css(header);
 $(".logo-line-before i, .logo-line-after i").css(logo_line_i);
 //$(".post code").css(post_code);
 $("#idhyt-surprise-ball #idhyt-surprise-ball-animation .drag").css(a_tag);
 $(".post-title-link, .posts-expand .post-meta, .post-comments-count, .disqus-comment-count, .post-category a, .post-nav-next a, .post-nav-item a").css(a_tag);
 
 // $("code").css({color: '#c5c8c6', background: '#1d1f21'});
 //$("#assist_btn1").hide(1500);
}

$(function () {
$("#assist_btn2").css("display","none");
 $("#assist_btn1").click(function() {
     switch_theme();
$("div#toc.toc-article").css({
 "background":"#eaeaea",
 "opacity":1
});
$(".toc-article ol").show();
$("#toc.toc-article .toc-title").css("color","#a98602");
$("#assist_btn1").css("display","none");
$("#assist_btn2").css("display","block");
 });
$("#assist_btn2").click(function() {
$("#assist_btn2").css("display","none");
$("#assist_btn1").css("display","block");
$("body").css("background","url(http://www.miaov.com/static/ie/images/news/bg.png)")
     $(".header, .footer").css("background","url(http://www.miaov.com/static/ie/images/news/bg.png)")
$(".toc-article ol").toggle(1000);
 });
});


//背景随机

var Y, O, E, L, B, C, T, z, N, S, A, I;
!function() {
var e = function() {
for (O.clearRect(0, 0, L, B), T = [{
x: 0,
y: .7 * B + C
}, {
x: 0,
y: .7 * B - C
}]; T[1].x < L + C;) t(T[0], T[1])
}, t = function(e, t) {
O.beginPath(), O.moveTo(e.x, e.y), O.lineTo(t.x, t.y);
var n = t.x + (2 * I() - .25) * C,
 r = a(t.y);
O.lineTo(n, r), O.closePath(), N -= S / -50, O.fillStyle = "#" + (127 * A(N) + 128 << 16 | 127 * A(N + S / 3) + 128 << 8 | 127 * A(N + S / 3 * 2) + 128).toString(16), O.fill(), T[0] = T[1], T[1] = {
 x: n,
 y: r
}
}, a = function n(e) {
var t = e + (2 * I() - 1.1) * C;
return t > B || t < 0 ? n(e) : t
};
Y = document.getElementById("evanyou"), O = Y.getContext("2d"), E = window.devicePixelRatio || 1, L = window.innerWidth, B = window.innerHeight, C = 90, z = Math, N = 0, S = 2 * z.PI, A = z.cos, I = z.random, Y.width = L * E, Y.height = B * E, O.scale(E, E), O.globalAlpha = .6, document.onclick = e, document.ontouchstart = e, e()
}()

   
$("#toc-eye").click(function(){
$("#toc.toc-article").toggle(1000);
});

</script>


   
  <div class="text-center donation">
    <div class="inner-donation">
      <span class="btn-donation">支持一下</span>
      <div class="donation-body">
        <div class="tip text-center">扫一扫，支持poetries</div>
        <ul>
        
          <li class="item">
            
              <span>微信扫一扫</span>
            
            <img src="/images/weixin.jpg" alt="">
          </li>
        
          <li class="item">
            
              <span>支付宝扫一扫</span>
            
            <img src="/images/zhifubao.jpg" alt="">
          </li>
        
        </ul>
      </div>
    </div>
  </div>


   
  <div class="box-prev-next clearfix">
    <a class="show pull-left" href="/2022/05/25/nest-summary/">
        <i class="icon icon-angle-left"></i>
    </a>
    <a class="show pull-right" href="/2022/06/17/nest-deploy-summary/">
        <i class="icon icon-angle-right"></i>
    </a>
  </div>




</div>


  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>




  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">关闭</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/archives/"
              rel="noopener noreferrer"
              target="_self"
              >
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/categories/"
              rel="noopener noreferrer"
              target="_self"
              >
              分类
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/tags/"
              rel="noopener noreferrer"
              target="_self"
              >
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/search/"
              rel="noopener noreferrer"
              target="_self"
              >
              搜索
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/link/"
              rel="noopener noreferrer"
              target="_self"
              >
              友链
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/about/"
              rel="noopener noreferrer"
              target="_self"
              >
              关于
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-post">
    

    

    
    

    

    
    

    

<!-- Gitalk评论插件通用代码 -->
<div id="gitalk-container"></div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script>
const gitalk = new Gitalk({
  clientID: '5567a2c4abb858009d96',
  clientSecret: 'b9039ec056cf5c2346b3cdb63308a28c163f91e5',
  repo: 'poetries.github.io',
  owner: 'poetries',
  // 在这里设置一下截取前50个字符串, 这是因为 github 对 label 的长度有了要求, 如果超过
  // 50个字符串则会报错.
  // id: location.pathname.split('/').pop().substring(0, 49),
  id: location.pathname,
  admin: ['poetries'],
  // facebook-like distraction free mode
  distractionFreeMode: false
})
gitalk.render('gitalk-container')
</script>
<!-- Gitalk代码结束 -->



  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script>


  <!-- 页面点击小红心 -->
  <script type="text/javascript" src="/js/clicklove.js"></script>
 
  
</body>
</html>
